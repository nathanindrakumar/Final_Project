{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "super-automation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime, date, timedelta\n",
    "import torch\n",
    "import skorch\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from skorch.helper import DataFrameTransformer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn.pipeline import Pipeline\n",
    "from skorch import NeuralNetRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-thomson",
   "metadata": {},
   "source": [
    "# Please ignore the below cells, they were for additional preprocessing and do not need to be run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "stuffed-daniel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('twitter_aapl_1.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "aapl = json.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "future-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "flataapl  = [val for sublist in aapl for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "primary-harvest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2021-05-11T10:18:35.000Z', 'id': '1392061565185236994', 'text': 'RT @Nicochan33: Apple Execs Chose to Keep a Hack of 128 Million iPhones Quiet https://t.co/waR9tgHKCA #tech #feedly #apple #iphone #cyberse‚Ä¶'}\n"
     ]
    }
   ],
   "source": [
    "print(flataapl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "twelve-polls",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>withheld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11T10:18:35.000Z</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11T10:18:30.000Z</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11T10:18:25.000Z</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11T10:17:54.000Z</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11T10:17:48.000Z</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18T07:25:14.000Z</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18T07:25:08.000Z</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18T07:25:06.000Z</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18T07:24:56.000Z</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18T07:24:54.000Z</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      created_at                   id  \\\n",
       "0       2021-05-11T10:18:35.000Z  1392061565185236994   \n",
       "1       2021-05-11T10:18:30.000Z  1392061546751217671   \n",
       "2       2021-05-11T10:18:25.000Z  1392061526614360066   \n",
       "3       2021-05-11T10:17:54.000Z  1392061393415909376   \n",
       "4       2021-05-11T10:17:48.000Z  1392061371886485506   \n",
       "...                          ...                  ...   \n",
       "517126  2021-02-18T07:25:14.000Z  1362302135333969929   \n",
       "517127  2021-02-18T07:25:08.000Z  1362302111027929089   \n",
       "517128  2021-02-18T07:25:06.000Z  1362302102404542467   \n",
       "517129  2021-02-18T07:24:56.000Z  1362302061589708805   \n",
       "517130  2021-02-18T07:24:54.000Z  1362302053889019904   \n",
       "\n",
       "                                                     text withheld  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha...      NaN  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr...      NaN  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia...      NaN  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK...      NaN  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...      NaN  \n",
       "...                                                   ...      ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New...      NaN  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...      NaN  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps...      NaN  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd      NaN  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...      NaN  \n",
       "\n",
       "[517131 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(flataapl)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "featured-electron",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152369    {'copyright': False, 'country_codes': ['IN']}\n",
       "257375    {'copyright': False, 'country_codes': ['IN']}\n",
       "269997    {'copyright': False, 'country_codes': ['IN']}\n",
       "315545    {'copyright': False, 'country_codes': ['RU']}\n",
       "315578    {'copyright': False, 'country_codes': ['RU']}\n",
       "343775    {'copyright': False, 'country_codes': ['IN']}\n",
       "473481    {'copyright': False, 'country_codes': ['RU']}\n",
       "473719    {'copyright': False, 'country_codes': ['RU']}\n",
       "489545    {'copyright': False, 'country_codes': ['IN']}\n",
       "509238    {'copyright': False, 'country_codes': ['IN']}\n",
       "Name: withheld, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['withheld'].dropna()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "excess-translation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11T10:18:35</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11T10:18:30</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11T10:18:25</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11T10:17:54</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11T10:17:48</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18T07:25:14</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18T07:25:08</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18T07:25:06</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18T07:24:56</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18T07:24:54</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id  \\\n",
       "0       2021-05-11T10:18:35  1392061565185236994   \n",
       "1       2021-05-11T10:18:30  1392061546751217671   \n",
       "2       2021-05-11T10:18:25  1392061526614360066   \n",
       "3       2021-05-11T10:17:54  1392061393415909376   \n",
       "4       2021-05-11T10:17:48  1392061371886485506   \n",
       "...                     ...                  ...   \n",
       "517126  2021-02-18T07:25:14  1362302135333969929   \n",
       "517127  2021-02-18T07:25:08  1362302111027929089   \n",
       "517128  2021-02-18T07:25:06  1362302102404542467   \n",
       "517129  2021-02-18T07:24:56  1362302061589708805   \n",
       "517130  2021-02-18T07:24:54  1362302053889019904   \n",
       "\n",
       "                                                     text  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha...  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr...  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia...  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK...  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...  \n",
       "...                                                   ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New...  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps...  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...  \n",
       "\n",
       "[517131 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.drop('withheld',axis=1)\n",
    "df1['created_at'] = df1['created_at'].map(lambda x: str(x)[:-5])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "certified-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"twitter_aapl_1_clean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "martial-consequence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                   id  \\\n",
       "0       2021-05-11  1392061565185236994   \n",
       "1       2021-05-11  1392061546751217671   \n",
       "2       2021-05-11  1392061526614360066   \n",
       "3       2021-05-11  1392061393415909376   \n",
       "4       2021-05-11  1392061371886485506   \n",
       "...            ...                  ...   \n",
       "517126  2021-02-18  1362302135333969929   \n",
       "517127  2021-02-18  1362302111027929089   \n",
       "517128  2021-02-18  1362302102404542467   \n",
       "517129  2021-02-18  1362302061589708805   \n",
       "517130  2021-02-18  1362302053889019904   \n",
       "\n",
       "                                                     text  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha...  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr...  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia...  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK...  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...  \n",
       "...                                                   ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New...  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps...  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...  \n",
       "\n",
       "[517131 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"twitter_aapl_1_clean.csv\")\n",
    "# Only including this step for now, basically removes the time component, leaving only date\n",
    "df1['created_at'] = df1['created_at'].map(lambda x: str(x)[:-9])\n",
    "df1 = df1.rename(columns={\"created_at\" : \"Date\"})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "elegant-deficit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14\n",
      "<class 'datetime.date'>\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://www.30secondsofcode.org/python/s/is-weekend\n",
    "def if_weekend(d = datetime.today()):\n",
    "    if d.weekday() == 5:\n",
    "        da = d + timedelta(days=2)\n",
    "        return da\n",
    "    elif d.weekday() == 6:\n",
    "        da = d + timedelta(days=1)\n",
    "        return da\n",
    "    elif d == date(2021,4,2):\n",
    "        return date(2021,4,5)\n",
    "    else:\n",
    "        return d\n",
    "print(if_weekend(date(2021,6,14)))\n",
    "print(type(date(2021,6,14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "general-nelson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>usedate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                   id  \\\n",
       "0      2021-05-11  1392061565185236994   \n",
       "1      2021-05-11  1392061546751217671   \n",
       "2      2021-05-11  1392061526614360066   \n",
       "3      2021-05-11  1392061393415909376   \n",
       "4      2021-05-11  1392061371886485506   \n",
       "...           ...                  ...   \n",
       "517126 2021-02-18  1362302135333969929   \n",
       "517127 2021-02-18  1362302111027929089   \n",
       "517128 2021-02-18  1362302102404542467   \n",
       "517129 2021-02-18  1362302061589708805   \n",
       "517130 2021-02-18  1362302053889019904   \n",
       "\n",
       "                                                     text    usedate  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha... 2021-05-11  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr... 2021-05-11  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia... 2021-05-11  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK... 2021-05-11  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #... 2021-05-11  \n",
       "...                                                   ...        ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New... 2021-02-18  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ... 2021-02-18  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps... 2021-02-18  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd 2021-02-18  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact... 2021-02-18  \n",
       "\n",
       "[517131 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Date']= pd.to_datetime(df1['Date'])\n",
    "df1['usedate'] = df1['Date'].map(lambda x: if_weekend(x))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "contained-forum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usedate</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.9100</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5000</td>\n",
       "      <td>126.2700</td>\n",
       "      <td>122.7700</td>\n",
       "      <td>2.4100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-10</td>\n",
       "      <td>126.8500</td>\n",
       "      <td>88071230</td>\n",
       "      <td>129.4100</td>\n",
       "      <td>129.5400</td>\n",
       "      <td>126.8100</td>\n",
       "      <td>-2.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-07</td>\n",
       "      <td>130.2100</td>\n",
       "      <td>78973270</td>\n",
       "      <td>130.8500</td>\n",
       "      <td>131.2582</td>\n",
       "      <td>129.4750</td>\n",
       "      <td>-0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>129.7400</td>\n",
       "      <td>78128330</td>\n",
       "      <td>127.8900</td>\n",
       "      <td>129.7500</td>\n",
       "      <td>127.1300</td>\n",
       "      <td>1.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-05</td>\n",
       "      <td>128.1000</td>\n",
       "      <td>84000900</td>\n",
       "      <td>129.2000</td>\n",
       "      <td>130.4500</td>\n",
       "      <td>127.9700</td>\n",
       "      <td>-1.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>78.7400</td>\n",
       "      <td>135372520</td>\n",
       "      <td>78.2925</td>\n",
       "      <td>79.1250</td>\n",
       "      <td>77.5810</td>\n",
       "      <td>0.4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>76.9275</td>\n",
       "      <td>166348360</td>\n",
       "      <td>75.0875</td>\n",
       "      <td>76.9750</td>\n",
       "      <td>75.0525</td>\n",
       "      <td>1.8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>77.3850</td>\n",
       "      <td>158929080</td>\n",
       "      <td>76.1275</td>\n",
       "      <td>77.4475</td>\n",
       "      <td>75.3825</td>\n",
       "      <td>1.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>76.9125</td>\n",
       "      <td>200622560</td>\n",
       "      <td>78.0375</td>\n",
       "      <td>78.9875</td>\n",
       "      <td>75.8025</td>\n",
       "      <td>-1.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>77.8525</td>\n",
       "      <td>162301040</td>\n",
       "      <td>79.4575</td>\n",
       "      <td>79.9220</td>\n",
       "      <td>77.7275</td>\n",
       "      <td>-1.6050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       usedate  Close/Last     Volume      Open      High       Low  Change\n",
       "0   2021-05-11    125.9100  126142800  123.5000  126.2700  122.7700  2.4100\n",
       "1   2021-05-10    126.8500   88071230  129.4100  129.5400  126.8100 -2.5600\n",
       "2   2021-05-07    130.2100   78973270  130.8500  131.2582  129.4750 -0.6400\n",
       "3   2021-05-06    129.7400   78128330  127.8900  129.7500  127.1300  1.8500\n",
       "4   2021-05-05    128.1000   84000900  129.2000  130.4500  127.9700 -1.1000\n",
       "..         ...         ...        ...       ...       ...       ...     ...\n",
       "247 2020-05-18     78.7400  135372520   78.2925   79.1250   77.5810  0.4475\n",
       "248 2020-05-15     76.9275  166348360   75.0875   76.9750   75.0525  1.8400\n",
       "249 2020-05-14     77.3850  158929080   76.1275   77.4475   75.3825  1.2575\n",
       "250 2020-05-13     76.9125  200622560   78.0375   78.9875   75.8025 -1.1250\n",
       "251 2020-05-12     77.8525  162301040   79.4575   79.9220   77.7275 -1.6050\n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = pd.read_csv(\"aapl2.csv\")\n",
    "price['Date'] = price['Date'].map(lambda x: parser.parse(x).isoformat())\n",
    "# Only including this step for now, basically removes the time component, leaving only date\n",
    "price['Date'] = price['Date'].map(lambda x: str(x)[:-9])\n",
    "price['Date']= pd.to_datetime(price['Date'])\n",
    "# Change me if looking to change examination scope\n",
    "price = price.rename(columns={\"Date\" : \"usedate\"})\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "filled-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>usedate</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                   id  \\\n",
       "0      2021-05-11  1392061565185236994   \n",
       "1      2021-05-11  1392061546751217671   \n",
       "2      2021-05-11  1392061526614360066   \n",
       "3      2021-05-11  1392061393415909376   \n",
       "4      2021-05-11  1392061371886485506   \n",
       "...           ...                  ...   \n",
       "517126 2021-02-18  1362302135333969929   \n",
       "517127 2021-02-18  1362302111027929089   \n",
       "517128 2021-02-18  1362302102404542467   \n",
       "517129 2021-02-18  1362302061589708805   \n",
       "517130 2021-02-18  1362302053889019904   \n",
       "\n",
       "                                                     text    usedate  \\\n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha... 2021-05-11   \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr... 2021-05-11   \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia... 2021-05-11   \n",
       "3       RT @iTech911: Future versions of #Apple's CarK... 2021-05-11   \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #... 2021-05-11   \n",
       "...                                                   ...        ...   \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New... 2021-02-18   \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ... 2021-02-18   \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps... 2021-02-18   \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd 2021-02-18   \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact... 2021-02-18   \n",
       "\n",
       "        Close/Last     Volume   Open     High     Low  Change  \n",
       "0           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "1           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "2           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "3           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "4           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "...            ...        ...    ...      ...     ...     ...  \n",
       "517126      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517127      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517128      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517129      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517130      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "\n",
       "[517131 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.merge(df1, price, how='left', on='usedate')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "overhead-truth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          0\n",
       "id            0\n",
       "text          0\n",
       "usedate       0\n",
       "Close/Last    0\n",
       "Volume        0\n",
       "Open          0\n",
       "High          0\n",
       "Low           0\n",
       "Change        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "imperial-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged.to_csv(\"merged.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-upper",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "every-boring",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  referenced_tweets  \\\n",
      "0        .\\nDomain Name for sale\\n\\nhttps://t.co/f056Wp...       1.391353e+18   \n",
      "1        .\\nDomain NameFor Sale\\n\\nhttps://t.co/a2N3rgJ...       1.391354e+18   \n",
      "2        .\\nDomains For sale\\n\\nhttps://t.co/0YeRs3JJLb...       1.391353e+18   \n",
      "3        .\\nDomain NameFor Sale\\n\\nhttps://t.co/a2N3rgJ...       1.391354e+18   \n",
      "4        Savanna Drinkers ü§¶\\n\\n#R500 #RHOA #AceMagashul...       1.392030e+18   \n",
      "...                                                    ...                ...   \n",
      "3387871  Download JOBIN THE PENGUIN! Awesome, retro-ins...                NaN   \n",
      "3387872  2019 13\" 1.6GHz/128GB #Apple #MacBook Airs, re...                NaN   \n",
      "3387873  How to use Apple Maps mobility trends data htt...                NaN   \n",
      "3387874  AirPods Studio Could Come With Head And Neck D...                NaN   \n",
      "3387875  Check it OUT Shades of Gray by Andy Holloman R...                NaN   \n",
      "\n",
      "                          id lang        Date      time     usedate  \\\n",
      "0        1392083055460851716   en  2021-05-11  11:43:58  2021-05-11   \n",
      "1        1392083046459969541   en  2021-05-11  11:43:56  2021-05-11   \n",
      "2        1392083045058940935   en  2021-05-11  11:43:56  2021-05-11   \n",
      "3        1392083027929358336   en  2021-05-11  11:43:52  2021-05-11   \n",
      "4        1392082985189511168   en  2021-05-11  11:43:41  2021-05-11   \n",
      "...                      ...  ...         ...       ...         ...   \n",
      "3387871  1259996823118319621   en  2020-05-12  00:00:26  2020-05-12   \n",
      "3387872  1259996818450132993   en  2020-05-12  00:00:25  2020-05-12   \n",
      "3387873  1259996729451085827   en  2020-05-12  00:00:04  2020-05-12   \n",
      "3387874  1259996725638504448   en  2020-05-12  00:00:03  2020-05-12   \n",
      "3387875  1259996722371219456   en  2020-05-12  00:00:02  2020-05-12   \n",
      "\n",
      "         Close/Last       Volume      Open     High       Low  Change  \n",
      "0          125.9100  126142800.0  123.5000  126.270  122.7700   2.410  \n",
      "1          125.9100  126142800.0  123.5000  126.270  122.7700   2.410  \n",
      "2          125.9100  126142800.0  123.5000  126.270  122.7700   2.410  \n",
      "3          125.9100  126142800.0  123.5000  126.270  122.7700   2.410  \n",
      "4          125.9100  126142800.0  123.5000  126.270  122.7700   2.410  \n",
      "...             ...          ...       ...      ...       ...     ...  \n",
      "3387871     77.8525  162301040.0   79.4575   79.922   77.7275  -1.605  \n",
      "3387872     77.8525  162301040.0   79.4575   79.922   77.7275  -1.605  \n",
      "3387873     77.8525  162301040.0   79.4575   79.922   77.7275  -1.605  \n",
      "3387874     77.8525  162301040.0   79.4575   79.922   77.7275  -1.605  \n",
      "3387875     77.8525  162301040.0   79.4575   79.922   77.7275  -1.605  \n",
      "\n",
      "[3387876 rows x 13 columns]\n",
      "0         2020 is almost ended\\n\\nBRAND NEW SEALED FACTO...\n",
      "1         If you feel like Apple has no love for you by ...\n",
      "2         üì¢NEW RIP IS LIVEüì¢\\n\\n2W1B 051 - iPhone, Capita...\n",
      "3         #Apple has introduced a new #macOS version of ...\n",
      "4         Good Dealüòç #MacBookPro #AppleEvent #Apple\\n\\nN...\n",
      "                                ...                        \n",
      "508177    Foreign Exchange History: How it All Started?\\...\n",
      "508178    Tropicana Apple Juice, 10 oz., 24 Count\\n\\nSta...\n",
      "508179    \"emotional conclusion to a dazzling series!\"\\n...\n",
      "508180    3ÔºÖË∂Ö‰∏äÊòá„Åó„ÅüÈäòÊüÑ\\n4434  „Çµ„Éº„Éê„Éº„ÉØ„Éº„ÇØ„Çπ\\n4625  „Ç¢„Éà„Éü„ÇØ„Çπ\\n6777  ...\n",
      "508181    EP 42: @seanmagers and @keithrconrad discuss t...\n",
      "Name: text, Length: 508182, dtype: object\n",
      "2020 is almost ended\n",
      "\n",
      "BRAND NEW SEALED FACTORY UNLOCKED iPHONE12 Mini to one LUCKY FOLLOWER.\n",
      "\n",
      "Keep Following &amp; Retweeting.\n",
      "\n",
      "#iTech911 #iBuy #iSell #iSwap #iFix #APPLE Ô£ø\n",
      "\n",
      "0262666226 üí¨ üìû https://t.co/3vOZQIN8yz\n",
      "If you feel like Apple has no love for you by not delivering the Heart Month Challenge badge, just restart your watch and wait for a few minutes. You‚Äôll get it!\n",
      "#apple #AppleWatch #challenge https://t.co/uWuvmVmRcX\n",
      "üì¢NEW RIP IS LIVEüì¢\n",
      "\n",
      "2W1B 051 - iPhone, Capitalism, Amazon Tax Gripes\n",
      "\n",
      "https://t.co/Tm4d6ftR57\n",
      "\n",
      "#Mondaythoughts #Mondaymorning #MondayMotivation #Hamont #BTC #Bitcoin #iphone12 #Apple $AAPL #eeftober #spotify #PS5 #Investing #iTunes #Sony $WEED $CGC #ETFs $AMZN $SPOT #nlpoli https://t.co/ckeMypyOLA\n"
     ]
    }
   ],
   "source": [
    "merged = pd.read_csv('mergedfull.csv')\n",
    "from sklearn.model_selection import train_test_split  \n",
    "mergedbig, mergedsmall = train_test_split(merged, test_size=0.15, random_state=0)\n",
    "# mergedsmall.to_csv(\"mergedsmall.csv\", index = False)\n",
    "mergedsmall = pd.read_csv('mergedsmall.csv')\n",
    "merged = mergedsmall\n",
    "# Obtaining the tweet contents into a list\n",
    "all_tweets = merged[\"text\"]\n",
    "print(all_tweets)\n",
    "all_tweets = all_tweets.to_list()\n",
    "print(all_tweets[0])\n",
    "print(all_tweets[1])\n",
    "print(all_tweets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nasty-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508182\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "according-eleven",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemming(tweet):\n",
    "    a = word_tokenize(tweet)\n",
    "    answer = list(map(lambda x: lemmatizer.lemmatize(x), a))\n",
    "    return answer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "muslim-driver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.gotrained.com/scraping-tweets-sentiment-analysis/\n",
    "# Basic cleaning of text before TF-IDF; this process will be improved later\n",
    "# for tweet in all_tweets:\n",
    "#     # Remove all the special characters\n",
    "#     processed_tweet = re.sub(r'\\W', ' ', tweet)\n",
    " \n",
    "#     # remove all single characters\n",
    "#     processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "#     # Remove single characters from the start\n",
    "#     processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "#     # Substituting multiple spaces with single space\n",
    "#     processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "#     # Removing prefixed 'b'\n",
    "#     processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    "    \n",
    "#     #removing common pronouns and prepositions\n",
    "#     processed_tweet = re.sub(r'of|to|https|keep|128', '', processed_tweet)\n",
    "#     processed_tweet = lemmatizer.lemmatize(processed_tweet)\n",
    "# #     lambda x: lemmatizer.lemmatize(x in processed_tweet)\n",
    "#     # Converting to Lowercase\n",
    "#     processed_tweet = processed_tweet.lower()\n",
    "    \n",
    "#     tweet = processed_tweet\n",
    "    \n",
    "    \n",
    "processed_tweets = []\n",
    "X = all_tweets\n",
    "for tweet in range(0, len(X)):  \n",
    "    \n",
    "\n",
    "    processed_tweet = re.sub(r\"http\\S+\", \"\", str(X[tweet]))\n",
    "\n",
    "    # Remove all the special characters\n",
    "    processed_tweet = re.sub(r'\\W', ' ', processed_tweet)\n",
    "    \n",
    "#     processed_tweet = re.sub(r'http\\S+', '', processed_tweet)\n",
    "    \n",
    "    \n",
    "#     processed_tweet = re.sub(r'co\\S+', '', processed_tweet) \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "    # Remove single characters from the start\n",
    "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "    # Removing prefixed 'b'\n",
    "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    "    \n",
    "\n",
    "    \n",
    "#     processed_tweet = re.sub(r'of|to|https|keep', '', processed_tweet)\n",
    "    # Converting to Lowercase\n",
    "    processed_tweet = processed_tweet.lower()\n",
    "    \n",
    "    processed_tweet = lemming(processed_tweet)\n",
    "    processed_tweet = TreebankWordDetokenizer().detokenize(processed_tweet)    \n",
    "    \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    "    processed_tweets.append(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "raised-master",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 is almost ended brand new sealed factory unlocked iphone12 mini to one lucky follower keep following amp retweeting itech911 ibuy isell iswap ifix apple 0262666226\n",
      "if you feel like apple ha no love for you by not delivering the heart month challenge badge just restart your watch and wait for few minute you ll get it apple applewatch challenge\n",
      "new rip is live 2w1b 051 iphone capitalism amazon tax gripe mondaythoughts mondaymorning mondaymotivation hamont btc bitcoin iphone12 apple aapl eeftober spotify ps5 investing itunes sony weed cgc etf amzn spot nlpoli\n"
     ]
    }
   ],
   "source": [
    "print(processed_tweets[0])\n",
    "print(processed_tweets[1])\n",
    "print(processed_tweets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "separated-flexibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.4424314246336387\n",
      "  (0, 33)\t0.1249567979802098\n",
      "  (0, 10)\t0.16582392918790628\n",
      "  (0, 17)\t0.11262468721438612\n",
      "  (0, 2)\t0.2080409113503352\n",
      "  (0, 18)\t0.10805227545782864\n",
      "  (0, 25)\t0.14608843790319945\n",
      "  (0, 20)\t0.18189356622112837\n",
      "  (0, 30)\t0.14506418279955546\n",
      "  (0, 34)\t0.16969490368608672\n",
      "  (0, 13)\t0.1531901222681532\n",
      "  (0, 16)\t0.17256851227177736\n",
      "  (0, 32)\t0.20371176864288223\n",
      "  (0, 27)\t0.1355941884983446\n",
      "  (0, 12)\t0.1266146098596461\n",
      "  (0, 14)\t0.15771445039678494\n",
      "  (0, 23)\t0.18061428764535165\n",
      "  (0, 15)\t0.20058649668753167\n",
      "  (0, 29)\t0.16542284403544966\n",
      "  (0, 24)\t0.18446483736773406\n",
      "  (0, 22)\t0.13564614142985149\n",
      "  (0, 21)\t0.17761529804847354\n",
      "  (0, 28)\t0.15788935531951526\n",
      "  (0, 0)\t0.22089715206910612\n",
      "  (0, 1)\t0.2514566660491283\n",
      "  :\t:\n",
      "  (508180, 1)\t0.041268949921978985\n",
      "  (508180, 3)\t0.06685152242931619\n",
      "  (508181, 5)\t0.20455669475441884\n",
      "  (508181, 31)\t0.11378430497461173\n",
      "  (508181, 17)\t0.21664203981153166\n",
      "  (508181, 18)\t0.2682199050847498\n",
      "  (508181, 25)\t0.18636512734943322\n",
      "  (508181, 20)\t0.1182946112357538\n",
      "  (508181, 30)\t0.09434259531556996\n",
      "  (508181, 34)\t0.26335431475373966\n",
      "  (508181, 13)\t0.18387335249459916\n",
      "  (508181, 16)\t0.20713313898185723\n",
      "  (508181, 32)\t0.10654890393570428\n",
      "  (508181, 27)\t0.22846327232174407\n",
      "  (508181, 12)\t0.17280792531215064\n",
      "  (508181, 14)\t0.21525404528771402\n",
      "  (508181, 23)\t0.21524684414985648\n",
      "  (508181, 15)\t0.27784951362680227\n",
      "  (508181, 29)\t0.22577472306465973\n",
      "  (508181, 24)\t0.25902960385333845\n",
      "  (508181, 22)\t0.210513078762402\n",
      "  (508181, 21)\t0.14972089906034738\n",
      "  (508181, 28)\t0.24327933232285534\n",
      "  (508181, 0)\t0.30148794518057503\n",
      "  (508181, 3)\t0.15646022425395406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "tfidfv = TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')\n",
    "df2 = tfidfv.fit_transform(processed_tweets)\n",
    "print(df2)\n",
    "# df2array = df2.toarray()\n",
    "# print(df2array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "announced-argument",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 556)\t0.40357727068148264\n",
      "  (0, 88)\t0.34308710016633726\n",
      "  (0, 97)\t0.417333363196858\n",
      "  (0, 443)\t0.3951839877481259\n",
      "  (0, 674)\t0.3663719517401211\n",
      "  (0, 1770)\t0.3199229308458355\n",
      "  (0, 1783)\t0.31050472625527564\n",
      "  (0, 943)\t0.15704072326243243\n",
      "  (0, 1716)\t0.15129041849415864\n",
      "  (0, 1471)\t0.09239185493528781\n"
     ]
    }
   ],
   "source": [
    "print(type(df2))\n",
    "print(type(df2[-1]))\n",
    "print(type(df2[-1][-1]))\n",
    "print(df2[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "arctic-drink",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6813202\n"
     ]
    }
   ],
   "source": [
    "print(df2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "auburn-interpretation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 943)\t0.1619164850216469\n",
      "  (0, 1667)\t0.24642359750991072\n",
      "  (0, 412)\t0.07984327940748012\n",
      "  (0, 863)\t0.07926270652557654\n",
      "  (0, 957)\t0.3644418579168204\n",
      "  (0, 1148)\t0.37252279088881424\n",
      "  (0, 24)\t0.44364094790152897\n",
      "  (0, 1243)\t0.19111331976332038\n",
      "  (0, 802)\t0.4405447542842643\n",
      "  (0, 1005)\t0.38259748006594846\n",
      "  (0, 1716)\t0.15598764620493955\n",
      "  (0, 189)\t0.14697293203863546\n",
      "  (0, 1471)\t0.09526041452797107\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(df2[0])\n",
    "\n",
    "print(df2array[0])\n",
    "print(type(df2[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-cannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df2array))\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(df2array[0])\n",
    "np.set_printoptions(threshold = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "reflected-playlist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt nicochan33 apple exec chose to keep hack of 128 million iphones quiet tech feedly apple iphone cyberse', 'rt roselovestyle house of the dragon the prequel to game of throne wa announced by casey bloys president of hbo gamesofthrones', 'rt gtorges ich habe jetzt einiges an material zu fuellmich amp co beim applesupport eingereicht hoffentlich verschwindet der podcast', 'rt itech911 future version of apple carkey could detect when it being used near wireless charger and alter how it work to avoid', 'Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åü apple airtag „Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ Âøò„Çå„ÇìÂùä ÂÆü„ÅØËøΩÂä†„Åß„ÇÇ„ÅÜ‰∏Ä„Å§Ë≤∑„Å£„Åü', 'rt nicochan33 apple exec chose to keep hack of 128 million iphones quiet tech feedly apple iphone cyberse', 'rt foxconcours_ apple macbook tente de gagner un macbookpro13 en participant ce concours pour tenter votre chance rt fo', 'rt taisy0 apple apple tv „Åßsiri„ÇíÂà©Áî®ÂèØËÉΩ„Å™ÂõΩ„ÇíÊã°Â§ß Ê∞ó„Å´„Å™„Çã Ë®ò„Å´„Å™„Çã apple', 'rt neodiycom how to fix apple macbookpro 2011 running windows10 10 64 bit audio sound driver problem macbook', 'rt indievideogames dino maker make dinos and level to share indiegame gamedev indiedev retro indievideogames io indiegames vi']\n"
     ]
    }
   ],
   "source": [
    "print(processed_tweets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "competent-witness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517131\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_tweets))\n",
    "print(type(processed_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "earlier-concentration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "enclosed-trial",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e96957eda679>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# https://programmerbackpack.com/tf-idf-explained-and-python-implementation/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidfv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TF-IDF\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdfnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TF-IDF'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdfnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# https://programmerbackpack.com/tf-idf-explained-and-python-implementation/\n",
    "dfnew = pd.DataFrame(df2[0].T.todense(), index=tfidfv.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "dfnew = dfnew.sort_values('TF-IDF', ascending=False)\n",
    "print (dfnew.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "presidential-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9     ...  1990  \\\n",
       "0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "517126   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517127   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517128   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517129   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517130   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "        1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "517126   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517127   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517128   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517129   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517130   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[517131 rows x 2000 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying an alternative data implementation, I haven't used this in the later stages yet\n",
    "df3 = pd.DataFrame.sparse.from_spmatrix(df2)\n",
    "print(type(df3))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "australian-sperm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#Selecting the data and splitting into train and test\n",
    "y = merged['Change']\n",
    "print(type(y))\n",
    "# X = df3\n",
    "# X = df2\n",
    "# X = df2array\n",
    "X = processed_tweets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "# train_data = train_data.sample(frac=x)\n",
    "# train_holdout_data.to_csv('train_holdout_data.csv', index=False)\n",
    "# x_holdout.to_csv('x_holdout.csv', index=False)\n",
    "# y_holdout.to_csv('y_holdout.csv', index=False)\n",
    "# train_data.to_csv('train_data.csv', index=False)\n",
    "\n",
    "# with open(\"x_train.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(x_train, fp)\n",
    "# with open(\"y_train.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(y_train, fp)\n",
    "# with open(\"x_test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(x_test, fp)\n",
    "# with open(\"y_test.txt\", \"wb\") as fp:   #Pickling\n",
    "#     pickle.dump(y_test, fp)\n",
    "\n",
    "with open(\"x_train.txt\", \"rb\") as fp:   # Unpickling\n",
    "    x_train = pickle.load(fp)\n",
    "with open(\"y_train.txt\", \"rb\") as fp:   # Unpickling\n",
    "    y_train = pickle.load(fp)\n",
    "with open(\"x_test.txt\", \"rb\") as fp:   # Unpickling\n",
    "    x_test = pickle.load(fp)\n",
    "with open(\"y_test.txt\", \"rb\") as fp:   # Unpickling\n",
    "    y_test = pickle.load(fp)\n",
    "\n",
    "# x_train.to_csv('x_train.csv', index=False)\n",
    "# y_train.to_csv('y_train.csv', index=False)\n",
    "# x_test.to_csv('x_test.csv', index=False)\n",
    "# y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "# # # train_holdout_data = pd.read_csv('train_holdout_data.csv')\n",
    "# # # x_holdout = pd.read_csv('x_holdout.csv')\n",
    "# # # y_holdout = pd.read_csv('y_holdout.csv')\n",
    "# x_train = pd.read_csv('x_train.csv')\n",
    "# y_train = pd.read_csv('y_train.csv')\n",
    "# x_test = pd.read_csv('x_test.csv')\n",
    "# y_test = pd.read_csv('y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "requested-router",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406545\n"
     ]
    }
   ],
   "source": [
    "# y_train = pd.Series(y_train)\n",
    "# y_test = pd.Series(y_test)\n",
    "print(len(x_train))\n",
    "# print(type(y_train.values))\n",
    "# print(np.isnan(X).sum())\n",
    "# print(y.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distant-slide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406545\n"
     ]
    }
   ],
   "source": [
    "# x_train = torch.from_numpy(x_train)\n",
    "# x_test = torch.from_numpy(x_test)\n",
    "# y_train = torch.from_numpy(y_train.values)\n",
    "# y_test = torch.from_numpy(y_test.values)\n",
    "print(len(x_train))\n",
    "# print(x_train)\n",
    "# print(x_test)\n",
    "# print(y_train)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "simplified-cause",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-42a2226c82f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)\n",
    "# print(x_test[100000][0])\n",
    "# if 'X' in x_train:\n",
    "#     print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "illegal-hearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.astype(np.float32)\n",
    "# y_test = y_test.astype(np.float32)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "# y_train = y_train.squeeze()\n",
    "# y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afraid-essence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "piano-martin",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-a8b81fe93964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# x_train1 = x_train.astype(np.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# y_train1 = y_train.astype(np.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtypes'"
     ]
    }
   ],
   "source": [
    "# x_train1 = x_train.astype(np.float32)\n",
    "# y_train1 = y_train.astype(np.float32)\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)\n",
    "print(x_train1.dtype)\n",
    "print(y_train1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "precise-cleveland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-7197c060c3b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "# for x in y_train[0]:\n",
    "#     if type(x) != \"class 'numpy.float32'\":\n",
    "#         print(\"double\")\n",
    "print(type(x_train[0]))\n",
    "print(type(y_train))\n",
    "print(type(y_train[0]))\n",
    "print(type(y_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "surprising-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainarray = y_train.values.tolist()\n",
    "y_testarray = y_test.values.tolist()\n",
    "# y_trainarray = y_train.to_numpy()\n",
    "# y_testarray = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dress-advertiser",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidfv = TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')\n",
    "x_train1 = tfidfv.fit_transform(x_train)\n",
    "x_test1 = tfidfv.fit_transform(x_test)\n",
    "x_train2 = x_train1.astype(dtype = np.float32)\n",
    "x_test2 = x_test1.astype(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "three-perception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5205\n",
      "(413704, 1)\n",
      "[[ 1.33]\n",
      " [-2.01]\n",
      " [ 1.14]\n",
      " ...\n",
      " [-3.29]\n",
      " [-2.73]\n",
      " [-1.33]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train2.shape[1])\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "seventh-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))\n",
    "# tokenise me pls\n",
    "a = [word_tokenize(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cathedral-quick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373108\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "print(len(set(flatten(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "posted-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.1850\u001b[0m        \u001b[32m4.0957\u001b[0m  70.8741\n",
      "      2        \u001b[36m4.0236\u001b[0m        \u001b[32m4.0082\u001b[0m  70.5556\n",
      "      3        \u001b[36m3.9830\u001b[0m        \u001b[32m3.9926\u001b[0m  73.4007\n",
      "      4        \u001b[36m3.9650\u001b[0m        \u001b[32m3.9773\u001b[0m  68.1609\n",
      "      5        \u001b[36m3.9520\u001b[0m        3.9899  70.5284\n",
      "      6        \u001b[36m3.9411\u001b[0m        \u001b[32m3.9743\u001b[0m  67.9473\n",
      "      7        \u001b[36m3.9332\u001b[0m        3.9756  67.6246\n",
      "      8        \u001b[36m3.9267\u001b[0m        \u001b[32m3.9669\u001b[0m  65.6628\n",
      "      9        \u001b[36m3.9200\u001b[0m        3.9674  64.3621\n",
      "     10        \u001b[36m3.9137\u001b[0m        \u001b[32m3.9612\u001b[0m  65.7217\n",
      "     11        \u001b[36m3.9090\u001b[0m        \u001b[32m3.9527\u001b[0m  70.4800\n",
      "     12        \u001b[36m3.9033\u001b[0m        \u001b[32m3.9524\u001b[0m  73.7520\n",
      "     13        \u001b[36m3.8986\u001b[0m        \u001b[32m3.9497\u001b[0m  73.8936\n",
      "     14        \u001b[36m3.8937\u001b[0m        3.9524  73.8280\n",
      "     15        \u001b[36m3.8897\u001b[0m        \u001b[32m3.9455\u001b[0m  74.1685\n",
      "     16        \u001b[36m3.8851\u001b[0m        \u001b[32m3.9444\u001b[0m  74.1716\n",
      "     17        \u001b[36m3.8813\u001b[0m        \u001b[32m3.9429\u001b[0m  74.1616\n",
      "     18        \u001b[36m3.8762\u001b[0m        3.9444  69.6191\n",
      "     19        \u001b[36m3.8730\u001b[0m        \u001b[32m3.9428\u001b[0m  68.3819\n",
      "     20        \u001b[36m3.8691\u001b[0m        \u001b[32m3.9409\u001b[0m  67.4868\n",
      "     21        \u001b[36m3.8666\u001b[0m        3.9412  67.1321\n",
      "     22        \u001b[36m3.8615\u001b[0m        \u001b[32m3.9404\u001b[0m  68.0268\n",
      "     23        \u001b[36m3.8580\u001b[0m        3.9416  69.9035\n",
      "     24        \u001b[36m3.8545\u001b[0m        \u001b[32m3.9392\u001b[0m  71.8225\n",
      "     25        \u001b[36m3.8511\u001b[0m        3.9395  68.8515\n",
      "     26        \u001b[36m3.8475\u001b[0m        \u001b[32m3.9381\u001b[0m  71.1462\n",
      "     27        \u001b[36m3.8446\u001b[0m        \u001b[32m3.9371\u001b[0m  78.2249\n",
      "     28        \u001b[36m3.8406\u001b[0m        3.9379  70.5178\n",
      "     29        \u001b[36m3.8373\u001b[0m        3.9390  68.1351\n",
      "     30        \u001b[36m3.8351\u001b[0m        \u001b[32m3.9325\u001b[0m  70.3013\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "in_dimension = 413704\n",
    "hid_dimension = 3\n",
    "out_dimension = 1\n",
    "\n",
    "\n",
    "class PoleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dimension,hid_dimension)\n",
    "        self.fc2 = nn.Linear(hid_dimension,out_dimension)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        hidden = self.fc1(X)\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        output = self.fc2(hidden)\n",
    "        return output\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "x_trainshape = 5842\n",
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "pole_model = RegressorModule()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(pole_model.parameters(), lr = 0.1)\n",
    "\n",
    "net = NeuralNetRegressor(module=pole_model, max_epochs=30, lr=0.1, callbacks =[('earlystopping',EarlyStopping())])\n",
    "def typechange(x):\n",
    "    return x.astype(dtype = np.float32)\n",
    "typetransform = FunctionTransformer(typechange)\n",
    "def inputneuron(x):\n",
    "    x_trainshape = x.shape[1]\n",
    "#     return x_trainshape\n",
    "inputneuronnumber = FunctionTransformer(inputneuron)\n",
    "# pipe = Pipeline([('transform', DataFrameTransformer()),('net', net)])\n",
    "# pipe = Pipeline([('net', net)])\n",
    "pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"typetransform\", typetransform), (\"net\", net)])\n",
    "\n",
    "\n",
    "# net = skorch.NeuralNetClassifier(module=PoleNN, max_epochs=20, lr=0.1, criterion=torch.nn.NLLLoss)\n",
    "pipe.fit(X=x_train, y=y_train)\n",
    "y_pred = pipe.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "freelance-craps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.971518"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "alone-moral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2394\u001b[0m        \u001b[32m4.2297\u001b[0m  72.4814\n",
      "      2        \u001b[36m4.1567\u001b[0m        \u001b[32m4.1073\u001b[0m  70.6289\n",
      "      3        \u001b[36m4.0670\u001b[0m        \u001b[32m4.0913\u001b[0m  72.2662\n",
      "      4        \u001b[36m4.0426\u001b[0m        \u001b[32m4.0363\u001b[0m  70.4901\n",
      "      5        \u001b[36m4.0162\u001b[0m        4.0882  70.8557\n",
      "      6        \u001b[36m4.0003\u001b[0m        4.0989  70.7887\n",
      "      7        \u001b[36m3.9910\u001b[0m        4.0867  71.7170\n",
      "      8        \u001b[36m3.9837\u001b[0m        4.0710  70.4721\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "5 RMSE: 2.0023932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2401\u001b[0m        \u001b[32m4.2351\u001b[0m  67.6599\n",
      "      2        \u001b[36m4.1687\u001b[0m        \u001b[32m4.0918\u001b[0m  69.9882\n",
      "      3        \u001b[36m4.0694\u001b[0m        \u001b[32m4.0911\u001b[0m  70.2413\n",
      "      4        \u001b[36m4.0430\u001b[0m        \u001b[32m4.0353\u001b[0m  67.6599\n",
      "      5        \u001b[36m4.0176\u001b[0m        4.0835  67.9217\n",
      "      6        \u001b[36m4.0010\u001b[0m        4.0947  68.4621\n",
      "      7        \u001b[36m3.9883\u001b[0m        4.0795  67.3682\n",
      "      8        \u001b[36m3.9796\u001b[0m        4.0551  68.7278\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "10 RMSE: 1.9989355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2416\u001b[0m        \u001b[32m4.2414\u001b[0m  71.0854\n",
      "      2        \u001b[36m4.1907\u001b[0m        \u001b[32m4.1663\u001b[0m  71.9156\n",
      "      3        \u001b[36m4.0650\u001b[0m        \u001b[32m4.0707\u001b[0m  71.7418\n",
      "      4        \u001b[36m4.0051\u001b[0m        \u001b[32m4.0529\u001b[0m  73.6199\n",
      "      5        \u001b[36m3.9785\u001b[0m        \u001b[32m4.0387\u001b[0m  74.5445\n",
      "      6        \u001b[36m3.9599\u001b[0m        \u001b[32m4.0109\u001b[0m  74.4860\n",
      "      7        \u001b[36m3.9452\u001b[0m        \u001b[32m4.0011\u001b[0m  72.7338\n",
      "      8        \u001b[36m3.9340\u001b[0m        \u001b[32m3.9897\u001b[0m  74.2163\n",
      "      9        \u001b[36m3.9245\u001b[0m        3.9910  73.9905\n",
      "     10        \u001b[36m3.9156\u001b[0m        \u001b[32m3.9788\u001b[0m  77.6048\n",
      "     11        \u001b[36m3.9071\u001b[0m        \u001b[32m3.9715\u001b[0m  80.6734\n",
      "     12        \u001b[36m3.8989\u001b[0m        3.9742  79.0142\n",
      "     13        \u001b[36m3.8918\u001b[0m        \u001b[32m3.9690\u001b[0m  73.4953\n",
      "     14        \u001b[36m3.8852\u001b[0m        \u001b[32m3.9654\u001b[0m  76.5282\n",
      "     15        \u001b[36m3.8782\u001b[0m        \u001b[32m3.9570\u001b[0m  76.6049\n",
      "     16        \u001b[36m3.8717\u001b[0m        \u001b[32m3.9564\u001b[0m  76.4651\n",
      "     17        \u001b[36m3.8648\u001b[0m        3.9569  76.1926\n",
      "     18        \u001b[36m3.8588\u001b[0m        \u001b[32m3.9553\u001b[0m  75.4409\n",
      "     19        \u001b[36m3.8527\u001b[0m        3.9587  72.4091\n",
      "     20        \u001b[36m3.8469\u001b[0m        3.9591  73.7127\n",
      "20 RMSE: 1.9807051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2388\u001b[0m        \u001b[32m4.2285\u001b[0m  79.2169\n",
      "      2        \u001b[36m4.1538\u001b[0m        \u001b[32m4.0881\u001b[0m  82.6719\n",
      "      3        \u001b[36m4.0580\u001b[0m        \u001b[32m4.0388\u001b[0m  80.6415\n",
      "      4        \u001b[36m4.0214\u001b[0m        4.0607  78.5574\n",
      "      5        \u001b[36m3.9960\u001b[0m        4.0691  79.6206\n",
      "      6        \u001b[36m3.9774\u001b[0m        4.0555  79.3489\n",
      "      7        \u001b[36m3.9633\u001b[0m        \u001b[32m4.0255\u001b[0m  83.3891\n",
      "      8        \u001b[36m3.9495\u001b[0m        \u001b[32m4.0152\u001b[0m  86.1895\n",
      "      9        \u001b[36m3.9376\u001b[0m        \u001b[32m4.0000\u001b[0m  82.5287\n",
      "     10        \u001b[36m3.9261\u001b[0m        \u001b[32m3.9926\u001b[0m  81.2770\n",
      "     11        \u001b[36m3.9152\u001b[0m        \u001b[32m3.9858\u001b[0m  76.3901\n",
      "     12        \u001b[36m3.9058\u001b[0m        \u001b[32m3.9794\u001b[0m  75.5199\n",
      "     13        \u001b[36m3.8961\u001b[0m        \u001b[32m3.9675\u001b[0m  75.6628\n",
      "     14        \u001b[36m3.8879\u001b[0m        \u001b[32m3.9488\u001b[0m  76.4510\n",
      "     15        \u001b[36m3.8784\u001b[0m        3.9522  75.5859\n",
      "     16        \u001b[36m3.8699\u001b[0m        \u001b[32m3.9445\u001b[0m  76.0484\n",
      "     17        \u001b[36m3.8612\u001b[0m        \u001b[32m3.9424\u001b[0m  77.9874\n",
      "     18        \u001b[36m3.8527\u001b[0m        \u001b[32m3.9357\u001b[0m  75.8865\n",
      "     19        \u001b[36m3.8443\u001b[0m        \u001b[32m3.9351\u001b[0m  74.9615\n",
      "     20        \u001b[36m3.8373\u001b[0m        3.9358  75.1563\n",
      "40 RMSE: 1.9745121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2396\u001b[0m        \u001b[32m4.2318\u001b[0m  77.2851\n",
      "      2        \u001b[36m4.1572\u001b[0m        \u001b[32m4.0760\u001b[0m  78.9254\n",
      "      3        \u001b[36m4.0541\u001b[0m        \u001b[32m4.0428\u001b[0m  79.0983\n",
      "      4        \u001b[36m4.0071\u001b[0m        4.0507  78.8875\n",
      "      5        \u001b[36m3.9791\u001b[0m        \u001b[32m4.0120\u001b[0m  79.5298\n",
      "      6        \u001b[36m3.9628\u001b[0m        \u001b[32m4.0018\u001b[0m  79.2811\n",
      "      7        \u001b[36m3.9483\u001b[0m        \u001b[32m3.9941\u001b[0m  79.8515\n",
      "      8        \u001b[36m3.9364\u001b[0m        \u001b[32m3.9899\u001b[0m  79.5398\n",
      "      9        \u001b[36m3.9252\u001b[0m        \u001b[32m3.9884\u001b[0m  79.9694\n",
      "     10        \u001b[36m3.9132\u001b[0m        3.9889  79.7476\n",
      "     11        \u001b[36m3.9021\u001b[0m        \u001b[32m3.9795\u001b[0m  80.4539\n",
      "     12        \u001b[36m3.8905\u001b[0m        \u001b[32m3.9694\u001b[0m  80.4579\n",
      "     13        \u001b[36m3.8787\u001b[0m        \u001b[32m3.9550\u001b[0m  80.7416\n",
      "     14        \u001b[36m3.8674\u001b[0m        \u001b[32m3.9468\u001b[0m  80.5318\n",
      "     15        \u001b[36m3.8566\u001b[0m        \u001b[32m3.9402\u001b[0m  80.9014\n",
      "     16        \u001b[36m3.8454\u001b[0m        \u001b[32m3.9303\u001b[0m  81.0353\n",
      "     17        \u001b[36m3.8341\u001b[0m        \u001b[32m3.9296\u001b[0m  80.7326\n",
      "     18        \u001b[36m3.8236\u001b[0m        \u001b[32m3.9207\u001b[0m  80.2201\n",
      "     19        \u001b[36m3.8134\u001b[0m        \u001b[32m3.9174\u001b[0m  80.7486\n",
      "     20        \u001b[36m3.8032\u001b[0m        \u001b[32m3.9174\u001b[0m  81.6966\n",
      "60 RMSE: 1.9710656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2385\u001b[0m        \u001b[32m4.2273\u001b[0m  78.9924\n",
      "      2        \u001b[36m4.1531\u001b[0m        \u001b[32m4.0923\u001b[0m  81.6567\n",
      "      3        \u001b[36m4.0585\u001b[0m        \u001b[32m4.0410\u001b[0m  81.7755\n",
      "      4        \u001b[36m4.0240\u001b[0m        4.0445  81.5857\n",
      "      5        \u001b[36m3.9996\u001b[0m        4.0733  83.1641\n",
      "      6        \u001b[36m3.9807\u001b[0m        4.0608  82.2281\n",
      "      7        \u001b[36m3.9653\u001b[0m        \u001b[32m4.0281\u001b[0m  82.5797\n",
      "      8        \u001b[36m3.9526\u001b[0m        \u001b[32m4.0105\u001b[0m  82.2950\n",
      "      9        \u001b[36m3.9415\u001b[0m        \u001b[32m3.9936\u001b[0m  86.7859\n",
      "     10        \u001b[36m3.9311\u001b[0m        \u001b[32m3.9923\u001b[0m  88.0426\n",
      "     11        \u001b[36m3.9206\u001b[0m        \u001b[32m3.9882\u001b[0m  89.2056\n",
      "     12        \u001b[36m3.9096\u001b[0m        3.9940  87.6016\n",
      "     13        \u001b[36m3.8988\u001b[0m        \u001b[32m3.9787\u001b[0m  90.7241\n",
      "     14        \u001b[36m3.8877\u001b[0m        \u001b[32m3.9711\u001b[0m  88.6361\n",
      "     15        \u001b[36m3.8764\u001b[0m        \u001b[32m3.9510\u001b[0m  89.7344\n",
      "     16        \u001b[36m3.8655\u001b[0m        \u001b[32m3.9487\u001b[0m  88.9202\n",
      "     17        \u001b[36m3.8552\u001b[0m        \u001b[32m3.9473\u001b[0m  90.1149\n",
      "     18        \u001b[36m3.8440\u001b[0m        \u001b[32m3.9381\u001b[0m  90.1402\n",
      "     19        \u001b[36m3.8322\u001b[0m        \u001b[32m3.9367\u001b[0m  89.9144\n",
      "     20        \u001b[36m3.8209\u001b[0m        \u001b[32m3.9320\u001b[0m  90.0512\n",
      "80 RMSE: 1.9754355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2391\u001b[0m        \u001b[32m4.2305\u001b[0m  91.1037\n",
      "      2        \u001b[36m4.1597\u001b[0m        \u001b[32m4.0905\u001b[0m  91.6197\n",
      "      3        \u001b[36m4.0615\u001b[0m        \u001b[32m4.0427\u001b[0m  93.8118\n",
      "      4        \u001b[36m4.0231\u001b[0m        4.0615  87.9053\n",
      "      5        \u001b[36m3.9935\u001b[0m        4.0699  86.8324\n",
      "      6        \u001b[36m3.9718\u001b[0m        \u001b[32m4.0330\u001b[0m  87.2140\n",
      "      7        \u001b[36m3.9548\u001b[0m        \u001b[32m4.0091\u001b[0m  86.8603\n",
      "      8        \u001b[36m3.9406\u001b[0m        \u001b[32m3.9947\u001b[0m  87.4048\n",
      "      9        \u001b[36m3.9273\u001b[0m        \u001b[32m3.9923\u001b[0m  89.1830\n",
      "     10        \u001b[36m3.9141\u001b[0m        \u001b[32m3.9760\u001b[0m  89.6615\n",
      "     11        \u001b[36m3.9023\u001b[0m        \u001b[32m3.9703\u001b[0m  91.9635\n",
      "     12        \u001b[36m3.8905\u001b[0m        \u001b[32m3.9540\u001b[0m  91.1741\n",
      "     13        \u001b[36m3.8788\u001b[0m        \u001b[32m3.9364\u001b[0m  98.2098\n",
      "     14        \u001b[36m3.8668\u001b[0m        \u001b[32m3.9312\u001b[0m  100.5317\n",
      "     15        \u001b[36m3.8557\u001b[0m        \u001b[32m3.9287\u001b[0m  102.9728\n",
      "     16        \u001b[36m3.8440\u001b[0m        \u001b[32m3.9212\u001b[0m  102.4360\n",
      "     17        \u001b[36m3.8328\u001b[0m        \u001b[32m3.9139\u001b[0m  103.2598\n",
      "     18        \u001b[36m3.8209\u001b[0m        \u001b[32m3.9078\u001b[0m  104.4199\n",
      "     19        \u001b[36m3.8086\u001b[0m        \u001b[32m3.9056\u001b[0m  103.0884\n",
      "     20        \u001b[36m3.7965\u001b[0m        \u001b[32m3.9020\u001b[0m  101.5969\n",
      "100 RMSE: 1.966813\n",
      "Optimum parameters: 100 : 1.966813\n"
     ]
    }
   ],
   "source": [
    "optimum_params = {'net__lr': 0.0015539382932245303, 'net__max_epochs': 20, 'net__optimizer__momentum': 0.9375124227337543}\n",
    "num_unitslist = [5,10,20,40,60,80,100]\n",
    "rmselist = []\n",
    "x_trainshape = 5842\n",
    "for num in num_unitslist:\n",
    "    class RegressorModule(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                num_units=num,\n",
    "                nonlin=F.relu,\n",
    "        ):\n",
    "            super(RegressorModule, self).__init__()\n",
    "            self.num_units = num_units\n",
    "            self.nonlin = nonlin\n",
    "\n",
    "            self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "            self.nonlin = nonlin\n",
    "            self.dense1 = nn.Linear(num_units, num)\n",
    "            self.output = nn.Linear(num, 1)\n",
    "\n",
    "        def forward(self, X, **kwargs):\n",
    "            X = self.nonlin(self.dense0(X))\n",
    "            X = F.relu(self.dense1(X))\n",
    "            X = self.output(X)\n",
    "            return X\n",
    "\n",
    "    pole_model = RegressorModule()\n",
    "\n",
    "    net = NeuralNetRegressor(module=pole_model, max_epochs=optimum_params['net__max_epochs'], lr=optimum_params['net__lr'], optimizer__momentum=optimum_params['net__optimizer__momentum'], callbacks =[('earlystopping',EarlyStopping())])\n",
    "    def typechange(x):\n",
    "        return x.astype(dtype = np.float32)\n",
    "    typetransform = FunctionTransformer(typechange)\n",
    "    def inputneuron(x):\n",
    "        x_trainshape = x.shape[1]\n",
    "    #     return x_trainshape\n",
    "    inputneuronnumber = FunctionTransformer(inputneuron)\n",
    "    # pipe = Pipeline([('transform', DataFrameTransformer()),('net', net)])\n",
    "    # pipe = Pipeline([('net', net)])\n",
    "    pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"typetransform\", typetransform), (\"net\", net)])\n",
    "\n",
    "\n",
    "    # net = skorch.NeuralNetClassifier(module=PoleNN, max_epochs=20, lr=0.1, criterion=torch.nn.NLLLoss)\n",
    "    pipe.fit(X=x_train, y=y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "    print(num,'RMSE:', rmse)\n",
    "    rmselist.append(rmse)\n",
    "\n",
    "min_index = rmselist.index((min(rmselist)))\n",
    "print(\"Optimum parameters:\", num_unitslist[min_index],\":\",min(rmselist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2400\u001b[0m        \u001b[32m4.2349\u001b[0m  92.0159\n",
      "      2        \u001b[36m4.1679\u001b[0m        \u001b[32m4.0870\u001b[0m  85.9619\n",
      "      3        \u001b[36m4.0655\u001b[0m        \u001b[32m4.0463\u001b[0m  86.0232\n",
      "      4        \u001b[36m4.0245\u001b[0m        4.0825  86.8104\n",
      "      5        \u001b[36m3.9895\u001b[0m        4.0646  86.8074\n",
      "      6        \u001b[36m3.9685\u001b[0m        \u001b[32m4.0370\u001b[0m  87.0012\n",
      "      7        \u001b[36m3.9520\u001b[0m        \u001b[32m4.0186\u001b[0m  87.0961\n",
      "      8        \u001b[36m3.9378\u001b[0m        \u001b[32m4.0053\u001b[0m  87.8753\n",
      "      9        \u001b[36m3.9241\u001b[0m        \u001b[32m3.9896\u001b[0m  88.3648\n",
      "     10        \u001b[36m3.9113\u001b[0m        \u001b[32m3.9748\u001b[0m  88.3633\n",
      "     11        \u001b[36m3.8975\u001b[0m        \u001b[32m3.9638\u001b[0m  88.4677\n",
      "     12        \u001b[36m3.8857\u001b[0m        \u001b[32m3.9472\u001b[0m  89.1810\n",
      "     13        \u001b[36m3.8727\u001b[0m        \u001b[32m3.9304\u001b[0m  88.9892\n",
      "     14        \u001b[36m3.8604\u001b[0m        \u001b[32m3.9296\u001b[0m  89.3078\n",
      "     15        \u001b[36m3.8487\u001b[0m        \u001b[32m3.9196\u001b[0m  89.7979\n",
      "     16        \u001b[36m3.8358\u001b[0m        \u001b[32m3.9140\u001b[0m  89.1660\n",
      "     17        \u001b[36m3.8247\u001b[0m        \u001b[32m3.9087\u001b[0m  90.3907\n",
      "     18        \u001b[36m3.8131\u001b[0m        \u001b[32m3.9043\u001b[0m  89.6515\n",
      "     19        \u001b[36m3.8003\u001b[0m        \u001b[32m3.9040\u001b[0m  89.7783\n",
      "     20        \u001b[36m3.7892\u001b[0m        \u001b[32m3.9036\u001b[0m  90.4496\n",
      "100 RMSE: 1.9672356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2404\u001b[0m        \u001b[32m4.2367\u001b[0m  93.9621\n",
      "      2        \u001b[36m4.1729\u001b[0m        \u001b[32m4.0900\u001b[0m  98.7672\n",
      "      3        \u001b[36m4.0641\u001b[0m        \u001b[32m4.0396\u001b[0m  100.5494\n",
      "      4        \u001b[36m4.0201\u001b[0m        4.0662  101.3396\n",
      "      5        \u001b[36m3.9864\u001b[0m        4.0495  102.4774\n",
      "      6        \u001b[36m3.9644\u001b[0m        \u001b[32m4.0194\u001b[0m  102.0998\n",
      "      7        \u001b[36m3.9469\u001b[0m        \u001b[32m4.0126\u001b[0m  103.4984\n",
      "      8        \u001b[36m3.9320\u001b[0m        \u001b[32m3.9969\u001b[0m  103.5693\n",
      "      9        \u001b[36m3.9183\u001b[0m        \u001b[32m3.9880\u001b[0m  104.2886\n",
      "     10        \u001b[36m3.9043\u001b[0m        \u001b[32m3.9684\u001b[0m  104.9279\n",
      "     11        \u001b[36m3.8921\u001b[0m        \u001b[32m3.9607\u001b[0m  105.2096\n",
      "     12        \u001b[36m3.8799\u001b[0m        \u001b[32m3.9442\u001b[0m  105.1257\n",
      "     13        \u001b[36m3.8668\u001b[0m        \u001b[32m3.9314\u001b[0m  105.1587\n",
      "     14        \u001b[36m3.8548\u001b[0m        \u001b[32m3.9287\u001b[0m  105.3505\n",
      "     15        \u001b[36m3.8428\u001b[0m        \u001b[32m3.9249\u001b[0m  106.5552\n",
      "     16        \u001b[36m3.8290\u001b[0m        \u001b[32m3.9216\u001b[0m  106.0408\n",
      "     17        \u001b[36m3.8154\u001b[0m        \u001b[32m3.9164\u001b[0m  106.3554\n",
      "     18        \u001b[36m3.8012\u001b[0m        3.9342  106.5722\n",
      "     19        \u001b[36m3.7894\u001b[0m        \u001b[32m3.9135\u001b[0m  107.3315\n",
      "     20        \u001b[36m3.7737\u001b[0m        3.9247  107.0697\n",
      "200 RMSE: 1.9712181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m4.2399\u001b[0m        \u001b[32m4.2345\u001b[0m  108.4603\n",
      "      2        \u001b[36m4.1665\u001b[0m        \u001b[32m4.0825\u001b[0m  115.7239\n",
      "      3        \u001b[36m4.0586\u001b[0m        \u001b[32m4.0405\u001b[0m  116.4242\n",
      "      4        \u001b[36m4.0103\u001b[0m        4.0741  117.6959\n",
      "      5        \u001b[36m3.9782\u001b[0m        4.0457  119.1364\n",
      "      6        \u001b[36m3.9587\u001b[0m        \u001b[32m4.0234\u001b[0m  119.7767\n",
      "      7        \u001b[36m3.9423\u001b[0m        \u001b[32m4.0097\u001b[0m  122.2073\n",
      "      8        \u001b[36m3.9273\u001b[0m        \u001b[32m4.0000\u001b[0m  121.8047\n",
      "      9        \u001b[36m3.9136\u001b[0m        \u001b[32m3.9838\u001b[0m  121.4321\n",
      "     10        \u001b[36m3.8992\u001b[0m        \u001b[32m3.9753\u001b[0m  142.2142\n",
      "     11        \u001b[36m3.8844\u001b[0m        \u001b[32m3.9637\u001b[0m  135.8511\n",
      "     12        \u001b[36m3.8698\u001b[0m        \u001b[32m3.9499\u001b[0m  137.0353\n",
      "     13        \u001b[36m3.8545\u001b[0m        \u001b[32m3.9335\u001b[0m  134.7927\n",
      "     14        \u001b[36m3.8404\u001b[0m        \u001b[32m3.9290\u001b[0m  143.1794\n",
      "     15        \u001b[36m3.8242\u001b[0m        \u001b[32m3.9115\u001b[0m  142.0846\n",
      "     16        \u001b[36m3.8088\u001b[0m        \u001b[32m3.9070\u001b[0m  130.8611\n",
      "     17        \u001b[36m3.7929\u001b[0m        \u001b[32m3.9064\u001b[0m  128.0619\n",
      "     18        \u001b[36m3.7783\u001b[0m        3.9166  128.3500\n",
      "     19        \u001b[36m3.7618\u001b[0m        3.9187  128.7826\n",
      "     20        \u001b[36m3.7442\u001b[0m        3.9224  128.9923\n",
      "300 RMSE: 1.971892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m4.2402\u001b[0m        \u001b[32m4.2361\u001b[0m  125.8431\n",
      "      2        \u001b[36m4.1690\u001b[0m        \u001b[32m4.0927\u001b[0m  137.5956\n",
      "      3        \u001b[36m4.0496\u001b[0m        \u001b[32m4.0618\u001b[0m  141.4147\n",
      "      4        \u001b[36m3.9983\u001b[0m        \u001b[32m4.0585\u001b[0m  140.2112\n",
      "      5        \u001b[36m3.9699\u001b[0m        \u001b[32m4.0304\u001b[0m  140.8877\n",
      "      6        \u001b[36m3.9504\u001b[0m        \u001b[32m4.0103\u001b[0m  142.7408\n",
      "      7        \u001b[36m3.9332\u001b[0m        \u001b[32m3.9957\u001b[0m  143.5305\n",
      "      8        \u001b[36m3.9182\u001b[0m        \u001b[32m3.9802\u001b[0m  145.4078\n",
      "      9        \u001b[36m3.9043\u001b[0m        \u001b[32m3.9672\u001b[0m  146.3578\n",
      "     10        \u001b[36m3.8901\u001b[0m        \u001b[32m3.9596\u001b[0m  148.8581\n",
      "     11        \u001b[36m3.8748\u001b[0m        \u001b[32m3.9399\u001b[0m  149.5834\n",
      "     12        \u001b[36m3.8603\u001b[0m        3.9402  149.5563\n",
      "     13        \u001b[36m3.8452\u001b[0m        \u001b[32m3.9158\u001b[0m  149.3066\n",
      "     14        \u001b[36m3.8287\u001b[0m        \u001b[32m3.9057\u001b[0m  149.4954\n",
      "     15        \u001b[36m3.8134\u001b[0m        3.9114  150.1148\n",
      "     16        \u001b[36m3.7973\u001b[0m        3.9133  151.4694\n",
      "     17        \u001b[36m3.7800\u001b[0m        \u001b[32m3.9032\u001b[0m  150.9599\n",
      "     18        \u001b[36m3.7646\u001b[0m        3.9087  150.8290\n",
      "     19        \u001b[36m3.7462\u001b[0m        3.9043  152.3475\n",
      "     20        \u001b[36m3.7311\u001b[0m        3.9144  152.0398\n",
      "400 RMSE: 1.9702251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m4.2399\u001b[0m        \u001b[32m4.2351\u001b[0m  156.5172\n",
      "      2        \u001b[36m4.1675\u001b[0m        \u001b[32m4.0858\u001b[0m  171.3881\n",
      "      3        \u001b[36m4.0518\u001b[0m        \u001b[32m4.0569\u001b[0m  173.1822\n",
      "      4        \u001b[36m4.0007\u001b[0m        4.0602  175.5458\n",
      "      5        \u001b[36m3.9710\u001b[0m        \u001b[32m4.0321\u001b[0m  178.8005\n",
      "      6        \u001b[36m3.9506\u001b[0m        \u001b[32m4.0164\u001b[0m  181.9323\n",
      "      7        \u001b[36m3.9329\u001b[0m        \u001b[32m3.9993\u001b[0m  182.9203\n",
      "      8        \u001b[36m3.9172\u001b[0m        \u001b[32m3.9828\u001b[0m  184.6835\n",
      "      9        \u001b[36m3.9022\u001b[0m        \u001b[32m3.9682\u001b[0m  186.1550\n",
      "     10        \u001b[36m3.8869\u001b[0m        \u001b[32m3.9595\u001b[0m  193.2193\n",
      "     11        \u001b[36m3.8714\u001b[0m        \u001b[32m3.9473\u001b[0m  195.5246\n",
      "     12        \u001b[36m3.8554\u001b[0m        \u001b[32m3.9248\u001b[0m  196.6167\n",
      "     13        \u001b[36m3.8388\u001b[0m        \u001b[32m3.9115\u001b[0m  197.8036\n",
      "     14        \u001b[36m3.8197\u001b[0m        \u001b[32m3.9078\u001b[0m  199.8828\n",
      "     15        \u001b[36m3.8015\u001b[0m        3.9106  200.4695\n",
      "     16        \u001b[36m3.7852\u001b[0m        \u001b[32m3.8969\u001b[0m  201.7175\n",
      "     17        \u001b[36m3.7648\u001b[0m        3.9003  202.7969\n",
      "     18        \u001b[36m3.7457\u001b[0m        3.9154  216.0335\n",
      "     19        \u001b[36m3.7268\u001b[0m        3.9220  218.1577\n",
      "     20        \u001b[36m3.7103\u001b[0m        3.9425  217.9451\n",
      "600 RMSE: 1.9759843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m4.2391\u001b[0m        \u001b[32m4.2313\u001b[0m  213.8890\n",
      "      2        \u001b[36m4.1584\u001b[0m        \u001b[32m4.0764\u001b[0m  229.8191\n",
      "      3        \u001b[36m4.0450\u001b[0m        \u001b[32m4.0637\u001b[0m  233.7261\n",
      "      4        \u001b[36m3.9954\u001b[0m        \u001b[32m4.0567\u001b[0m  229.0162\n",
      "      5        \u001b[36m3.9667\u001b[0m        \u001b[32m4.0350\u001b[0m  232.6707\n",
      "      6        \u001b[36m3.9468\u001b[0m        \u001b[32m4.0185\u001b[0m  234.1069\n",
      "      7        \u001b[36m3.9288\u001b[0m        \u001b[32m3.9933\u001b[0m  240.0113\n",
      "      8        \u001b[36m3.9130\u001b[0m        \u001b[32m3.9696\u001b[0m  243.5466\n",
      "      9        \u001b[36m3.8967\u001b[0m        \u001b[32m3.9589\u001b[0m  256.2258\n",
      "     10        \u001b[36m3.8825\u001b[0m        \u001b[32m3.9462\u001b[0m  240.1301\n",
      "     11        \u001b[36m3.8670\u001b[0m        \u001b[32m3.9271\u001b[0m  258.7098\n",
      "     12        \u001b[36m3.8487\u001b[0m        \u001b[32m3.9254\u001b[0m  236.7620\n",
      "     13        \u001b[36m3.8332\u001b[0m        3.9296  236.2105\n",
      "     14        \u001b[36m3.8158\u001b[0m        \u001b[32m3.9235\u001b[0m  233.9563\n",
      "     15        \u001b[36m3.7968\u001b[0m        \u001b[32m3.9062\u001b[0m  236.0631\n",
      "     16        \u001b[36m3.7778\u001b[0m        3.9125  238.1893\n",
      "     17        \u001b[36m3.7584\u001b[0m        3.9163  244.1232\n",
      "     18        \u001b[36m3.7387\u001b[0m        3.9474  244.0335\n",
      "     19        \u001b[36m3.7224\u001b[0m        3.9516  247.8189\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "800 RMSE: 1.9796176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss       dur\n",
      "-------  ------------  ------------  --------\n",
      "      1        \u001b[36m4.2381\u001b[0m        \u001b[32m4.2269\u001b[0m  224.4839\n",
      "      2        \u001b[36m4.1489\u001b[0m        \u001b[32m4.0696\u001b[0m  245.9677\n",
      "      3        \u001b[36m4.0397\u001b[0m        4.0700  248.6130\n",
      "      4        \u001b[36m3.9918\u001b[0m        \u001b[32m4.0525\u001b[0m  251.7387\n",
      "      5        \u001b[36m3.9643\u001b[0m        \u001b[32m4.0327\u001b[0m  255.7215\n",
      "      6        \u001b[36m3.9435\u001b[0m        \u001b[32m4.0125\u001b[0m  260.7315\n",
      "      7        \u001b[36m3.9271\u001b[0m        \u001b[32m3.9862\u001b[0m  261.2781\n",
      "      8        \u001b[36m3.9108\u001b[0m        \u001b[32m3.9743\u001b[0m  263.9237\n",
      "      9        \u001b[36m3.8942\u001b[0m        \u001b[32m3.9611\u001b[0m  266.4313\n"
     ]
    }
   ],
   "source": [
    "optimum_params = {'net__lr': 0.0015539382932245303, 'net__max_epochs': 20, 'net__optimizer__momentum': 0.9375124227337543}\n",
    "num_unitslist = [100,200,300,400,600,800,1000,2000]\n",
    "rmselist = []\n",
    "x_trainshape = 5842\n",
    "for num in num_unitslist:\n",
    "    class RegressorModule(nn.Module):\n",
    "        def __init__(\n",
    "                self,\n",
    "                num_units=num,\n",
    "                nonlin=F.relu,\n",
    "        ):\n",
    "            super(RegressorModule, self).__init__()\n",
    "            self.num_units = num_units\n",
    "            self.nonlin = nonlin\n",
    "\n",
    "            self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "            self.nonlin = nonlin\n",
    "            self.dense1 = nn.Linear(num_units, num)\n",
    "            self.output = nn.Linear(num, 1)\n",
    "\n",
    "        def forward(self, X, **kwargs):\n",
    "            X = self.nonlin(self.dense0(X))\n",
    "            X = F.relu(self.dense1(X))\n",
    "            X = self.output(X)\n",
    "            return X\n",
    "\n",
    "    pole_model = RegressorModule()\n",
    "\n",
    "    net = NeuralNetRegressor(module=pole_model, max_epochs=optimum_params['net__max_epochs'], lr=optimum_params['net__lr'], optimizer__momentum=optimum_params['net__optimizer__momentum'], callbacks =[('earlystopping',EarlyStopping())])\n",
    "    def typechange(x):\n",
    "        return x.astype(dtype = np.float32)\n",
    "    typetransform = FunctionTransformer(typechange)\n",
    "    def inputneuron(x):\n",
    "        x_trainshape = x.shape[1]\n",
    "    #     return x_trainshape\n",
    "    inputneuronnumber = FunctionTransformer(inputneuron)\n",
    "    # pipe = Pipeline([('transform', DataFrameTransformer()),('net', net)])\n",
    "    # pipe = Pipeline([('net', net)])\n",
    "    pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"typetransform\", typetransform), (\"net\", net)])\n",
    "\n",
    "\n",
    "    # net = skorch.NeuralNetClassifier(module=PoleNN, max_epochs=20, lr=0.1, criterion=torch.nn.NLLLoss)\n",
    "    pipe.fit(X=x_train, y=y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "    print(num,'RMSE:', rmse)\n",
    "    rmselist.append(rmse)\n",
    "\n",
    "min_index = rmselist.index((min(rmselist)))\n",
    "print(\"Optimum parameters:\", num_unitslist[min_index],\":\",min(rmselist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "appropriate-exchange",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71518.68]\n",
      "[35764.336]\n"
     ]
    }
   ],
   "source": [
    "profit = 0\n",
    "for i, (real, pred) in enumerate(zip(y_test, y_pred)):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        realchange = y_test[i] - y_test[i-1]\n",
    "        invest  = (y_pred[i] - y_pred[i-1])\n",
    "        change = (invest * realchange)\n",
    "        profit += change\n",
    "print(profit)\n",
    "profit = 0           \n",
    "for i, (real, pred) in enumerate(zip(y_test, y_pred)):\n",
    "    realmean = y_test.mean()\n",
    "    predmean = y_pred.mean()\n",
    "    realchange = real - realmean\n",
    "    predchange = pred - predmean\n",
    "    change = (predchange * realchange)\n",
    "    profit += change\n",
    "print(profit)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "particular-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scipy.stats._distn_infrastructure.rv_frozen object at 0x00000246D1FE4D90>\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "print(uniform(20, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optional-captain",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8071\u001b[0m        \u001b[32m3.8260\u001b[0m  51.2935\n",
      "      2        \u001b[36m3.7947\u001b[0m        3.8399  51.5512\n",
      "      3        \u001b[36m3.7888\u001b[0m        3.8498  51.6012\n",
      "      4        \u001b[36m3.7835\u001b[0m        3.8568  52.3434\n",
      "      5        \u001b[36m3.7780\u001b[0m        3.8655  51.8709\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8190\u001b[0m        \u001b[32m3.9146\u001b[0m  51.4773\n",
      "      2        \u001b[36m3.8059\u001b[0m        \u001b[32m3.9112\u001b[0m  51.8639\n",
      "      3        \u001b[36m3.7988\u001b[0m        3.9343  52.0787\n",
      "      4        \u001b[36m3.7930\u001b[0m        3.9281  52.5502\n",
      "      5        \u001b[36m3.7889\u001b[0m        3.9176  52.0397\n",
      "      6        \u001b[36m3.7838\u001b[0m        3.9211  51.9718\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8266\u001b[0m        \u001b[32m3.9289\u001b[0m  51.4523\n",
      "      2        \u001b[36m3.8154\u001b[0m        \u001b[32m3.9252\u001b[0m  52.3564\n",
      "      3        \u001b[36m3.8070\u001b[0m        \u001b[32m3.9100\u001b[0m  51.6252\n",
      "      4        \u001b[36m3.8006\u001b[0m        3.9159  52.0278\n",
      "      5        \u001b[36m3.7969\u001b[0m        3.9256  52.2226\n",
      "      6        \u001b[36m3.7921\u001b[0m        3.9314  52.1246\n",
      "      7        \u001b[36m3.7867\u001b[0m        3.9319  52.3195\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8173\u001b[0m        \u001b[32m3.9047\u001b[0m  51.3115\n",
      "      2        \u001b[36m3.8044\u001b[0m        3.9126  52.3594\n",
      "      3        \u001b[36m3.7976\u001b[0m        3.9213  51.6651\n",
      "      4        \u001b[36m3.7906\u001b[0m        3.9209  52.3384\n",
      "      5        \u001b[36m3.7860\u001b[0m        3.9157  52.6102\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8104\u001b[0m        \u001b[32m3.9196\u001b[0m  51.7940\n",
      "      2        \u001b[36m3.7974\u001b[0m        3.9375  52.2116\n",
      "      3        \u001b[36m3.7900\u001b[0m        3.9397  51.8380\n",
      "      4        \u001b[36m3.7832\u001b[0m        3.9362  52.3255\n",
      "      5        \u001b[36m3.7795\u001b[0m        3.9407  52.5123\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8450\u001b[0m        \u001b[32m3.8743\u001b[0m  51.5342\n",
      "      2        \u001b[36m3.8289\u001b[0m        3.8917  51.7670\n",
      "      3        \u001b[36m3.8230\u001b[0m        3.8805  52.0547\n",
      "      4        \u001b[36m3.8130\u001b[0m        \u001b[32m3.8728\u001b[0m  52.0367\n",
      "      5        \u001b[36m3.8031\u001b[0m        \u001b[32m3.8565\u001b[0m  52.5552\n",
      "      6        \u001b[36m3.7952\u001b[0m        3.8701  52.3145\n",
      "      7        \u001b[36m3.7931\u001b[0m        3.8684  52.2465\n",
      "      8        \u001b[36m3.7865\u001b[0m        3.8623  52.1247\n",
      "      9        \u001b[36m3.7830\u001b[0m        3.8615  52.2855\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8532\u001b[0m        \u001b[32m3.9413\u001b[0m  50.9069\n",
      "      2        \u001b[36m3.8362\u001b[0m        \u001b[32m3.9281\u001b[0m  51.8569\n",
      "      3        \u001b[36m3.8270\u001b[0m        3.9391  51.7830\n",
      "      4        \u001b[36m3.8169\u001b[0m        3.9503  51.8010\n",
      "      5        \u001b[36m3.8110\u001b[0m        3.9546  51.7940\n",
      "      6        \u001b[36m3.8069\u001b[0m        3.9606  52.3354\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8619\u001b[0m        \u001b[32m3.9467\u001b[0m  51.3055\n",
      "      2        \u001b[36m3.8493\u001b[0m        3.9664  51.7960\n",
      "      3        \u001b[36m3.8362\u001b[0m        \u001b[32m3.9442\u001b[0m  52.1127\n",
      "      4        \u001b[36m3.8274\u001b[0m        \u001b[32m3.9360\u001b[0m  51.9099\n",
      "      5        \u001b[36m3.8176\u001b[0m        \u001b[32m3.9273\u001b[0m  52.4323\n",
      "      6        \u001b[36m3.8118\u001b[0m        3.9477  52.4333\n",
      "      7        \u001b[36m3.8063\u001b[0m        3.9363  52.0737\n",
      "      8        \u001b[36m3.8004\u001b[0m        3.9559  52.5133\n",
      "      9        \u001b[36m3.7960\u001b[0m        3.9503  51.5712\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8546\u001b[0m        \u001b[32m3.9312\u001b[0m  52.1436\n",
      "      2        \u001b[36m3.8337\u001b[0m        3.9403  52.1776\n",
      "      3        \u001b[36m3.8256\u001b[0m        3.9435  52.1536\n",
      "      4        \u001b[36m3.8173\u001b[0m        3.9361  51.6981\n",
      "      5        \u001b[36m3.8074\u001b[0m        \u001b[32m3.9260\u001b[0m  51.9019\n",
      "      6        \u001b[36m3.8013\u001b[0m        \u001b[32m3.9238\u001b[0m  52.1376\n",
      "      7        \u001b[36m3.7981\u001b[0m        3.9375  52.4004\n",
      "      8        \u001b[36m3.7913\u001b[0m        3.9496  51.7650\n",
      "      9        \u001b[36m3.7854\u001b[0m        3.9437  51.6641\n",
      "     10        \u001b[36m3.7807\u001b[0m        3.9638  51.9718\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8455\u001b[0m        \u001b[32m3.9573\u001b[0m  51.3954\n",
      "      2        \u001b[36m3.8260\u001b[0m        \u001b[32m3.9422\u001b[0m  51.9968\n",
      "      3        \u001b[36m3.8176\u001b[0m        \u001b[32m3.9299\u001b[0m  52.1546\n",
      "      4        \u001b[36m3.8091\u001b[0m        3.9518  51.8270\n",
      "      5        \u001b[36m3.8005\u001b[0m        3.9410  51.7740\n",
      "      6        \u001b[36m3.7954\u001b[0m        3.9640  52.6002\n",
      "      7        \u001b[36m3.7873\u001b[0m        3.9417  51.9438\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8227\u001b[0m        \u001b[32m3.8559\u001b[0m  51.2136\n",
      "      2        \u001b[36m3.8122\u001b[0m        \u001b[32m3.8478\u001b[0m  51.7191\n",
      "      3        \u001b[36m3.8045\u001b[0m        3.8634  51.7610\n",
      "      4        \u001b[36m3.7983\u001b[0m        3.8701  52.2096\n",
      "      5        \u001b[36m3.7907\u001b[0m        3.8647  51.9209\n",
      "      6        \u001b[36m3.7840\u001b[0m        3.8745  51.9039\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8355\u001b[0m        \u001b[32m3.9321\u001b[0m  51.6831\n",
      "      2        \u001b[36m3.8215\u001b[0m        3.9347  51.8010\n",
      "      3        \u001b[36m3.8098\u001b[0m        3.9372  51.8210\n",
      "      4        \u001b[36m3.8057\u001b[0m        3.9642  51.7361\n",
      "      5        \u001b[36m3.7980\u001b[0m        3.9384  52.5472\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8437\u001b[0m        \u001b[32m3.9367\u001b[0m  52.3574\n",
      "      2        \u001b[36m3.8289\u001b[0m        3.9368  52.2705\n",
      "      3        \u001b[36m3.8202\u001b[0m        3.9424  52.2795\n",
      "      4        \u001b[36m3.8139\u001b[0m        \u001b[32m3.9276\u001b[0m  51.7151\n",
      "      5        \u001b[36m3.8072\u001b[0m        \u001b[32m3.9226\u001b[0m  52.0467\n",
      "      6        \u001b[36m3.8004\u001b[0m        3.9249  52.0058\n",
      "      7        \u001b[36m3.7969\u001b[0m        3.9339  52.4363\n",
      "      8        \u001b[36m3.7918\u001b[0m        3.9396  51.8080\n",
      "      9        \u001b[36m3.7872\u001b[0m        3.9312  52.2475\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8343\u001b[0m        \u001b[32m3.9377\u001b[0m  51.5103\n",
      "      2        \u001b[36m3.8213\u001b[0m        \u001b[32m3.9255\u001b[0m  51.7240\n",
      "      3        \u001b[36m3.8108\u001b[0m        \u001b[32m3.9205\u001b[0m  52.0917\n",
      "      4        \u001b[36m3.8044\u001b[0m        3.9299  51.5682\n",
      "      5        \u001b[36m3.7995\u001b[0m        3.9458  51.8000\n",
      "      6        \u001b[36m3.7918\u001b[0m        3.9385  51.7890\n",
      "      7        \u001b[36m3.7878\u001b[0m        3.9321  51.8160\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8245\u001b[0m        \u001b[32m3.9438\u001b[0m  51.2735\n",
      "      2        \u001b[36m3.8110\u001b[0m        \u001b[32m3.9308\u001b[0m  51.5632\n",
      "      3        \u001b[36m3.8034\u001b[0m        \u001b[32m3.9246\u001b[0m  52.6042\n",
      "      4        \u001b[36m3.7939\u001b[0m        3.9252  51.8340\n",
      "      5        \u001b[36m3.7874\u001b[0m        \u001b[32m3.9229\u001b[0m  52.0178\n",
      "      6        \u001b[36m3.7823\u001b[0m        3.9395  51.6332\n",
      "      7        \u001b[36m3.7790\u001b[0m        3.9317  52.4643\n",
      "      8        \u001b[36m3.7733\u001b[0m        3.9603  51.7660\n",
      "      9        \u001b[36m3.7689\u001b[0m        3.9619  52.0877\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.0433\u001b[0m        \u001b[32m3.9715\u001b[0m  52.2285\n",
      "      2        \u001b[36m4.0104\u001b[0m        \u001b[32m3.9708\u001b[0m  51.9259\n",
      "      3        \u001b[36m4.0040\u001b[0m        3.9755  52.2046\n",
      "      4        \u001b[36m3.9941\u001b[0m        3.9721  52.4793\n",
      "      5        \u001b[36m3.9839\u001b[0m        \u001b[32m3.9623\u001b[0m  52.3474\n",
      "      6        \u001b[36m3.9815\u001b[0m        3.9714  52.2845\n",
      "      7        \u001b[36m3.9788\u001b[0m        3.9655  52.1107\n",
      "      8        \u001b[36m3.9760\u001b[0m        3.9801  52.7560\n",
      "      9        \u001b[36m3.9737\u001b[0m        3.9666  52.6531\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2188\u001b[0m        \u001b[32m4.2394\u001b[0m  51.8539\n",
      "      2        4.2795        4.2394  53.0537\n",
      "      3        4.2795        4.2394  53.1017\n",
      "      4        4.2795        4.2394  53.0237\n",
      "      5        4.2795        4.2394  53.3504\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.1015\u001b[0m        \u001b[32m4.0139\u001b[0m  51.7061\n",
      "      2        \u001b[36m4.0982\u001b[0m        \u001b[32m4.0076\u001b[0m  51.8909\n",
      "      3        \u001b[36m4.0248\u001b[0m        \u001b[32m3.9764\u001b[0m  52.7181\n",
      "      4        \u001b[36m4.0032\u001b[0m        3.9999  51.9998\n",
      "      5        \u001b[36m3.9882\u001b[0m        \u001b[32m3.9640\u001b[0m  52.2455\n",
      "      6        3.9892        \u001b[32m3.9609\u001b[0m  52.3304\n",
      "      7        4.0519        3.9676  51.8819\n",
      "      8        \u001b[36m3.9696\u001b[0m        3.9627  52.5952\n",
      "      9        4.0314        3.9851  52.9288\n",
      "     10        3.9713        3.9697  52.2465\n",
      "     11        \u001b[36m3.9636\u001b[0m        \u001b[32m3.9498\u001b[0m  52.2046\n",
      "     12        \u001b[36m3.9453\u001b[0m        3.9564  51.9049\n",
      "     13        \u001b[36m3.9430\u001b[0m        \u001b[32m3.9484\u001b[0m  52.4723\n",
      "     14        \u001b[36m3.9344\u001b[0m        3.9488  52.4793\n",
      "     15        3.9404        3.9521  52.5912\n",
      "     16        3.9386        3.9497  52.1576\n",
      "     17        \u001b[36m3.9295\u001b[0m        3.9550  52.1057\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.1532\u001b[0m        \u001b[32m4.1242\u001b[0m  51.8929\n",
      "      2        \u001b[36m4.1388\u001b[0m        \u001b[32m4.0184\u001b[0m  52.6351\n",
      "      3        \u001b[36m4.0148\u001b[0m        \u001b[32m3.9905\u001b[0m  52.4134\n",
      "      4        4.0237        4.0076  52.6541\n",
      "      5        \u001b[36m4.0033\u001b[0m        3.9946  52.1097\n",
      "      6        \u001b[36m3.9971\u001b[0m        \u001b[32m3.9752\u001b[0m  52.1426\n",
      "      7        \u001b[36m3.9873\u001b[0m        3.9854  52.6701\n",
      "      8        \u001b[36m3.9736\u001b[0m        \u001b[32m3.9690\u001b[0m  52.4573\n",
      "      9        \u001b[36m3.9603\u001b[0m        \u001b[32m3.9611\u001b[0m  52.2745\n",
      "     10        4.0330        3.9791  52.8809\n",
      "     11        3.9718        4.0052  52.6921\n",
      "     12        3.9637        3.9906  53.0607\n",
      "     13        3.9645        3.9649  52.9888\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.1461\u001b[0m        \u001b[32m4.2895\u001b[0m  51.3604\n",
      "      2        \u001b[36m4.0726\u001b[0m        \u001b[32m4.2487\u001b[0m  52.2525\n",
      "      3        4.1374        \u001b[32m4.1648\u001b[0m  52.3324\n",
      "      4        4.1411        4.1693  52.5782\n",
      "      5        \u001b[36m4.0426\u001b[0m        4.2154  52.4693\n",
      "      6        \u001b[36m4.0172\u001b[0m        \u001b[32m4.0934\u001b[0m  51.6172\n",
      "      7        \u001b[36m4.0122\u001b[0m        \u001b[32m4.0776\u001b[0m  52.3974\n",
      "      8        \u001b[36m3.9724\u001b[0m        4.1787  52.5282\n",
      "      9        3.9844        \u001b[32m4.0495\u001b[0m  51.6911\n",
      "     10        \u001b[36m3.9620\u001b[0m        4.1106  52.3794\n",
      "     11        \u001b[36m3.9492\u001b[0m        4.0843  52.2545\n",
      "     12        \u001b[36m3.9439\u001b[0m        4.0916  52.3474\n",
      "     13        \u001b[36m3.9433\u001b[0m        4.1067  52.7071\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9876\u001b[0m        \u001b[32m3.9508\u001b[0m  51.8260\n",
      "      2        \u001b[36m3.9428\u001b[0m        3.9737  53.1926\n",
      "      3        \u001b[36m3.9264\u001b[0m        3.9529  52.8000\n",
      "      4        \u001b[36m3.9136\u001b[0m        \u001b[32m3.9198\u001b[0m  52.4743\n",
      "      5        \u001b[36m3.9038\u001b[0m        \u001b[32m3.9014\u001b[0m  52.6002\n",
      "      6        \u001b[36m3.8963\u001b[0m        3.9099  52.5522\n",
      "      7        \u001b[36m3.8860\u001b[0m        3.9165  52.8369\n",
      "      8        \u001b[36m3.8792\u001b[0m        3.9296  52.0168\n",
      "      9        \u001b[36m3.8747\u001b[0m        3.9266  52.7480\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9698\u001b[0m        \u001b[32m3.9723\u001b[0m  51.8160\n",
      "      2        \u001b[36m3.9450\u001b[0m        3.9837  52.4224\n",
      "      3        \u001b[36m3.9230\u001b[0m        \u001b[32m3.9628\u001b[0m  53.9848\n",
      "      4        \u001b[36m3.9079\u001b[0m        \u001b[32m3.9502\u001b[0m  52.9348\n",
      "      5        \u001b[36m3.8937\u001b[0m        3.9546  53.3114\n",
      "      6        \u001b[36m3.8829\u001b[0m        \u001b[32m3.9440\u001b[0m  52.9178\n",
      "      7        \u001b[36m3.8776\u001b[0m        \u001b[32m3.9427\u001b[0m  53.0617\n",
      "      8        \u001b[36m3.8670\u001b[0m        \u001b[32m3.9380\u001b[0m  53.1176\n",
      "      9        \u001b[36m3.8606\u001b[0m        3.9381  52.3334\n",
      "     10        \u001b[36m3.8543\u001b[0m        3.9414  52.8809\n",
      "     11        \u001b[36m3.8474\u001b[0m        \u001b[32m3.9373\u001b[0m  52.6152\n",
      "     12        \u001b[36m3.8396\u001b[0m        \u001b[32m3.9285\u001b[0m  52.7450\n",
      "     13        \u001b[36m3.8354\u001b[0m        3.9353  53.1226\n",
      "     14        \u001b[36m3.8351\u001b[0m        3.9311  52.5672\n",
      "     15        \u001b[36m3.8317\u001b[0m        3.9384  52.8759\n",
      "     16        \u001b[36m3.8291\u001b[0m        3.9410  52.8299\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9733\u001b[0m        \u001b[32m3.9642\u001b[0m  52.1147\n",
      "      2        \u001b[36m3.9555\u001b[0m        3.9696  52.7031\n",
      "      3        \u001b[36m3.9341\u001b[0m        \u001b[32m3.9528\u001b[0m  52.5712\n",
      "      4        \u001b[36m3.9131\u001b[0m        3.9592  52.5822\n",
      "      5        \u001b[36m3.8954\u001b[0m        3.9562  52.6661\n",
      "      6        \u001b[36m3.8836\u001b[0m        \u001b[32m3.9306\u001b[0m  52.8979\n",
      "      7        \u001b[36m3.8727\u001b[0m        3.9975  52.7380\n",
      "      8        \u001b[36m3.8677\u001b[0m        3.9466  52.3275\n",
      "      9        \u001b[36m3.8552\u001b[0m        3.9510  52.7470\n",
      "     10        \u001b[36m3.8489\u001b[0m        3.9625  52.3724\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9555\u001b[0m        \u001b[32m4.0601\u001b[0m  51.8289\n",
      "      2        \u001b[36m3.9236\u001b[0m        \u001b[32m3.9557\u001b[0m  52.1117\n",
      "      3        \u001b[36m3.9099\u001b[0m        \u001b[32m3.9514\u001b[0m  52.3914\n",
      "      4        \u001b[36m3.8889\u001b[0m        \u001b[32m3.9455\u001b[0m  52.6751\n",
      "      5        \u001b[36m3.8730\u001b[0m        \u001b[32m3.9308\u001b[0m  52.6032\n",
      "      6        \u001b[36m3.8633\u001b[0m        \u001b[32m3.9273\u001b[0m  52.5103\n",
      "      7        \u001b[36m3.8539\u001b[0m        3.9301  53.3954\n",
      "      8        \u001b[36m3.8439\u001b[0m        \u001b[32m3.9179\u001b[0m  52.9039\n",
      "      9        \u001b[36m3.8397\u001b[0m        3.9479  53.1776\n",
      "     10        \u001b[36m3.8296\u001b[0m        3.9397  52.4513\n",
      "     11        \u001b[36m3.8256\u001b[0m        3.9482  52.4224\n",
      "     12        \u001b[36m3.8198\u001b[0m        3.9497  52.5662\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9468\u001b[0m        \u001b[32m3.9966\u001b[0m  51.8259\n",
      "      2        \u001b[36m3.9168\u001b[0m        4.0739  52.4903\n",
      "      3        \u001b[36m3.8851\u001b[0m        4.0238  52.1976\n",
      "      4        \u001b[36m3.8690\u001b[0m        4.0053  52.5562\n",
      "      5        \u001b[36m3.8619\u001b[0m        4.0419  52.1107\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9976\u001b[0m        \u001b[32m3.9399\u001b[0m  51.2725\n",
      "      2        \u001b[36m3.9790\u001b[0m        3.9865  52.1616\n",
      "      3        \u001b[36m3.9788\u001b[0m        3.9835  52.6941\n",
      "      4        \u001b[36m3.9603\u001b[0m        3.9743  53.0737\n",
      "      5        \u001b[36m3.9512\u001b[0m        \u001b[32m3.9312\u001b[0m  52.6162\n",
      "      6        \u001b[36m3.9480\u001b[0m        3.9525  52.9488\n",
      "      7        \u001b[36m3.9390\u001b[0m        3.9558  53.2445\n",
      "      8        3.9392        3.9753  53.0767\n",
      "      9        \u001b[36m3.9289\u001b[0m        3.9890  53.1676\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9831\u001b[0m        \u001b[32m4.0121\u001b[0m  51.1157\n",
      "      2        \u001b[36m3.9687\u001b[0m        \u001b[32m3.9986\u001b[0m  52.8000\n",
      "      3        \u001b[36m3.9558\u001b[0m        \u001b[32m3.9737\u001b[0m  53.1846\n",
      "      4        \u001b[36m3.9419\u001b[0m        3.9950  52.1037\n",
      "      5        \u001b[36m3.9255\u001b[0m        3.9760  52.8959\n",
      "      6        \u001b[36m3.9152\u001b[0m        \u001b[32m3.9650\u001b[0m  52.3664\n",
      "      7        \u001b[36m3.9069\u001b[0m        \u001b[32m3.9589\u001b[0m  52.9838\n",
      "      8        3.9135        \u001b[32m3.9456\u001b[0m  53.2865\n",
      "      9        3.9240        3.9725  53.1176\n",
      "     10        3.9284        3.9592  54.9847\n",
      "     11        3.9143        3.9494  53.0717\n",
      "     12        \u001b[36m3.9032\u001b[0m        4.0021  53.3924\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9922\u001b[0m        \u001b[32m4.0032\u001b[0m  51.5862\n",
      "      2        \u001b[36m3.9771\u001b[0m        \u001b[32m3.9687\u001b[0m  53.1206\n",
      "      3        \u001b[36m3.9650\u001b[0m        4.0130  54.7150\n",
      "      4        3.9660        4.0002  54.1935\n",
      "      5        \u001b[36m3.9491\u001b[0m        3.9861  52.5512\n",
      "      6        3.9550        4.0593  52.5572\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9742\u001b[0m        \u001b[32m4.0292\u001b[0m  51.0727\n",
      "      2        \u001b[36m3.9658\u001b[0m        \u001b[32m3.9820\u001b[0m  52.1436\n",
      "      3        \u001b[36m3.9640\u001b[0m        4.0183  52.3005\n",
      "      4        \u001b[36m3.9409\u001b[0m        4.0501  52.2595\n",
      "      5        \u001b[36m3.9338\u001b[0m        \u001b[32m3.9747\u001b[0m  52.5602\n",
      "      6        \u001b[36m3.9173\u001b[0m        \u001b[32m3.9729\u001b[0m  51.9798\n",
      "      7        \u001b[36m3.9121\u001b[0m        \u001b[32m3.9612\u001b[0m  51.6911\n",
      "      8        \u001b[36m3.8990\u001b[0m        \u001b[32m3.9492\u001b[0m  52.2445\n",
      "      9        \u001b[36m3.8941\u001b[0m        \u001b[32m3.9426\u001b[0m  52.2425\n",
      "     10        \u001b[36m3.8817\u001b[0m        3.9978  52.5782\n",
      "     11        3.8830        3.9867  52.5462\n",
      "     12        \u001b[36m3.8805\u001b[0m        3.9549  51.9488\n",
      "     13        \u001b[36m3.8730\u001b[0m        3.9997  52.4473\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.9718\u001b[0m        \u001b[32m4.0100\u001b[0m  51.1486\n",
      "      2        \u001b[36m3.9688\u001b[0m        4.0317  52.5093\n",
      "      3        \u001b[36m3.9446\u001b[0m        \u001b[32m3.9882\u001b[0m  52.3205\n",
      "      4        \u001b[36m3.9319\u001b[0m        \u001b[32m3.9797\u001b[0m  51.5672\n",
      "      5        \u001b[36m3.9205\u001b[0m        4.0028  52.3065\n",
      "      6        \u001b[36m3.9074\u001b[0m        \u001b[32m3.9690\u001b[0m  52.2925\n",
      "      7        \u001b[36m3.9029\u001b[0m        \u001b[32m3.9677\u001b[0m  52.1237\n",
      "      8        \u001b[36m3.8939\u001b[0m        \u001b[32m3.9644\u001b[0m  52.7840\n",
      "      9        \u001b[36m3.8847\u001b[0m        \u001b[32m3.9531\u001b[0m  51.8859\n",
      "     10        \u001b[36m3.8809\u001b[0m        3.9808  52.5362\n",
      "     11        \u001b[36m3.8786\u001b[0m        3.9770  52.3035\n",
      "     12        \u001b[36m3.8738\u001b[0m        3.9899  52.5932\n",
      "     13        \u001b[36m3.8637\u001b[0m        3.9818  53.2865\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3122\u001b[0m        \u001b[32m4.2602\u001b[0m  51.3035\n",
      "      2        4.3141        4.2602  52.1057\n",
      "      3        4.3141        4.2602  52.2775\n",
      "      4        4.3141        4.2602  51.9608\n",
      "      5        4.3141        4.2602  51.6911\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3110\u001b[0m        \u001b[32m4.2757\u001b[0m  51.4334\n",
      "      2        4.3155        4.2757  51.9958\n",
      "      3        4.3155        4.2757  52.0308\n",
      "      4        4.3155        4.2757  52.2625\n",
      "      5        4.3155        4.2757  51.3894\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3111\u001b[0m        \u001b[32m4.2757\u001b[0m  51.5472\n",
      "      2        4.3177        4.2757  52.1636\n",
      "      3        4.3177        4.2757  52.0767\n",
      "      4        4.3177        4.2757  51.8779\n",
      "      5        4.3177        4.2757  52.1746\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3022\u001b[0m        \u001b[32m4.2757\u001b[0m  51.8919\n",
      "      2        4.3088        4.2757  51.9608\n",
      "      3        4.3088        \u001b[32m4.2757\u001b[0m  52.3364\n",
      "      4        4.3088        \u001b[32m4.2757\u001b[0m  51.6721\n",
      "      5        4.3089        4.2757  52.6042\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2934\u001b[0m        \u001b[32m4.2854\u001b[0m  51.8529\n",
      "      2        4.3007        \u001b[32m4.2854\u001b[0m  51.9768\n",
      "      3        4.3007        \u001b[32m4.2851\u001b[0m  51.7870\n",
      "      4        4.2995        4.2854  51.9069\n",
      "      5        4.3007        4.2854  52.3314\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3950\u001b[0m        \u001b[32m4.2636\u001b[0m  51.6182\n",
      "      2        4.3967        4.2636  52.4883\n",
      "      3        4.3967        4.2636  51.6921\n",
      "      4        4.3967        4.2636  52.1466\n",
      "      5        4.3967        4.2636  51.6382\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3993\u001b[0m        \u001b[32m4.2793\u001b[0m  51.6621\n",
      "      2        4.4034        4.2793  51.5782\n",
      "      3        4.4034        4.2793  51.7231\n",
      "      4        4.4034        4.2793  51.5842\n",
      "      5        4.4034        4.2793  51.9159\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.4032\u001b[0m        \u001b[32m4.2793\u001b[0m  51.9159\n",
      "      2        4.4073        4.2793  51.4753\n",
      "      3        4.4073        4.2793  51.3964\n",
      "      4        4.4073        4.2793  51.7830\n",
      "      5        4.4073        4.2793  51.3844\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3833\u001b[0m        \u001b[32m4.2793\u001b[0m  51.5712\n",
      "      2        4.3874        4.2793  51.6581\n",
      "      3        4.3874        4.2793  51.7351\n",
      "      4        4.3874        4.2793  51.3724\n",
      "      5        4.3874        4.2793  52.1656\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3847\u001b[0m        \u001b[32m4.2461\u001b[0m  51.4124\n",
      "      2        4.3897        4.2461  51.6052\n",
      "      3        4.3897        4.2461  51.7321\n",
      "      4        4.3897        4.2461  51.8080\n",
      "      5        4.3897        4.2461  50.8410\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7706\u001b[0m        \u001b[32m3.8046\u001b[0m  51.2785\n",
      "      2        \u001b[36m3.7636\u001b[0m        3.8064  51.6921\n",
      "      3        \u001b[36m3.7594\u001b[0m        3.8067  51.5552\n",
      "      4        \u001b[36m3.7560\u001b[0m        3.8093  51.8919\n",
      "      5        \u001b[36m3.7526\u001b[0m        3.8110  52.0048\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7819\u001b[0m        \u001b[32m3.9042\u001b[0m  51.6801\n",
      "      2        \u001b[36m3.7751\u001b[0m        3.9050  51.6192\n",
      "      3        \u001b[36m3.7710\u001b[0m        3.9065  52.4094\n",
      "      4        \u001b[36m3.7677\u001b[0m        3.9050  51.5652\n",
      "      5        \u001b[36m3.7642\u001b[0m        3.9049  51.8729\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7920\u001b[0m        \u001b[32m3.9041\u001b[0m  51.0308\n",
      "      2        \u001b[36m3.7849\u001b[0m        3.9051  51.5762\n",
      "      3        \u001b[36m3.7805\u001b[0m        3.9042  51.3674\n",
      "      4        \u001b[36m3.7770\u001b[0m        3.9052  51.3634\n",
      "      5        \u001b[36m3.7736\u001b[0m        3.9059  51.4024\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7846\u001b[0m        \u001b[32m3.9069\u001b[0m  51.5093\n",
      "      2        \u001b[36m3.7773\u001b[0m        \u001b[32m3.9065\u001b[0m  51.4474\n",
      "      3        \u001b[36m3.7729\u001b[0m        \u001b[32m3.9063\u001b[0m  51.4953\n",
      "      4        \u001b[36m3.7690\u001b[0m        3.9078  51.7810\n",
      "      5        \u001b[36m3.7655\u001b[0m        3.9089  52.1057\n",
      "      6        \u001b[36m3.7623\u001b[0m        3.9105  51.2526\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7789\u001b[0m        \u001b[32m3.9177\u001b[0m  51.0158\n",
      "      2        \u001b[36m3.7712\u001b[0m        \u001b[32m3.9175\u001b[0m  51.4593\n",
      "      3        \u001b[36m3.7665\u001b[0m        \u001b[32m3.9169\u001b[0m  52.0028\n",
      "      4        \u001b[36m3.7624\u001b[0m        3.9172  51.8639\n",
      "      5        \u001b[36m3.7589\u001b[0m        3.9198  51.6002\n",
      "      6        \u001b[36m3.7557\u001b[0m        3.9200  51.1277\n",
      "      7        \u001b[36m3.7528\u001b[0m        3.9203  51.7760\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.0298\u001b[0m        \u001b[32m3.9457\u001b[0m  51.9428\n",
      "      2        \u001b[36m3.9930\u001b[0m        3.9572  52.8229\n",
      "      3        \u001b[36m3.9738\u001b[0m        \u001b[32m3.9321\u001b[0m  52.2146\n",
      "      4        \u001b[36m3.9517\u001b[0m        3.9411  52.2116\n",
      "      5        \u001b[36m3.9465\u001b[0m        \u001b[32m3.9223\u001b[0m  52.3964\n",
      "      6        \u001b[36m3.9398\u001b[0m        3.9284  52.7430\n",
      "      7        \u001b[36m3.9311\u001b[0m        3.9314  52.7360\n",
      "      8        \u001b[36m3.9245\u001b[0m        3.9226  52.4313\n",
      "      9        \u001b[36m3.9187\u001b[0m        3.9254  52.4024\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.0465\u001b[0m        \u001b[32m3.9840\u001b[0m  51.5123\n",
      "      2        \u001b[36m4.0012\u001b[0m        \u001b[32m3.9758\u001b[0m  52.1886\n",
      "      3        \u001b[36m3.9869\u001b[0m        4.0032  52.0807\n",
      "      4        \u001b[36m3.9804\u001b[0m        4.0589  52.0727\n",
      "      5        \u001b[36m3.9754\u001b[0m        3.9900  52.3394\n",
      "      6        \u001b[36m3.9677\u001b[0m        3.9904  52.2935\n",
      "      7        \u001b[36m3.9568\u001b[0m        \u001b[32m3.9591\u001b[0m  52.2515\n",
      "      8        \u001b[36m3.9491\u001b[0m        3.9654  51.9858\n",
      "      9        \u001b[36m3.9407\u001b[0m        \u001b[32m3.9544\u001b[0m  52.2675\n",
      "     10        \u001b[36m3.9341\u001b[0m        3.9588  52.8080\n",
      "     11        \u001b[36m3.9280\u001b[0m        3.9593  52.5153\n",
      "     12        \u001b[36m3.9245\u001b[0m        3.9649  52.7081\n",
      "     13        \u001b[36m3.9203\u001b[0m        3.9591  52.5322\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.0500\u001b[0m        \u001b[32m4.0027\u001b[0m  51.6811\n",
      "      2        \u001b[36m4.0040\u001b[0m        \u001b[32m3.9836\u001b[0m  52.0667\n",
      "      3        \u001b[36m3.9902\u001b[0m        3.9978  52.1237\n",
      "      4        \u001b[36m3.9874\u001b[0m        \u001b[32m3.9649\u001b[0m  52.7830\n",
      "      5        \u001b[36m3.9678\u001b[0m        \u001b[32m3.9632\u001b[0m  52.1926\n",
      "      6        3.9697        \u001b[32m3.9491\u001b[0m  52.7430\n",
      "      7        \u001b[36m3.9609\u001b[0m        3.9703  52.6401\n",
      "      8        \u001b[36m3.9513\u001b[0m        3.9721  52.3564\n",
      "      9        \u001b[36m3.9392\u001b[0m        3.9597  52.5542\n",
      "     10        \u001b[36m3.9303\u001b[0m        3.9719  51.9588\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.0427\u001b[0m        \u001b[32m4.0144\u001b[0m  51.3145\n",
      "      2        \u001b[36m3.9993\u001b[0m        \u001b[32m3.9863\u001b[0m  52.4753\n",
      "      3        \u001b[36m3.9805\u001b[0m        \u001b[32m3.9630\u001b[0m  52.2595\n",
      "      4        \u001b[36m3.9654\u001b[0m        3.9683  52.2885\n",
      "      5        3.9719        3.9743  52.7760\n",
      "      6        \u001b[36m3.9553\u001b[0m        3.9804  52.7940\n",
      "      7        \u001b[36m3.9420\u001b[0m        3.9783  52.2675\n",
      "      8        \u001b[36m3.9341\u001b[0m        \u001b[32m3.9536\u001b[0m  52.1386\n",
      "      9        \u001b[36m3.9310\u001b[0m        3.9662  52.8060\n",
      "     10        \u001b[36m3.9239\u001b[0m        3.9827  52.0647\n",
      "     11        \u001b[36m3.9191\u001b[0m        3.9537  52.4473\n",
      "     12        \u001b[36m3.9160\u001b[0m        3.9549  52.3115\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.0381\u001b[0m        \u001b[32m4.1137\u001b[0m  51.5413\n",
      "      2        \u001b[36m3.9981\u001b[0m        4.1232  52.0567\n",
      "      3        \u001b[36m3.9722\u001b[0m        \u001b[32m4.0383\u001b[0m  51.9428\n",
      "      4        \u001b[36m3.9580\u001b[0m        \u001b[32m4.0183\u001b[0m  52.2685\n",
      "      5        \u001b[36m3.9470\u001b[0m        4.0280  52.1466\n",
      "      6        \u001b[36m3.9388\u001b[0m        4.0280  52.3324\n",
      "      7        \u001b[36m3.9366\u001b[0m        4.0378  51.8769\n",
      "      8        \u001b[36m3.9331\u001b[0m        4.0723  52.7131\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8921\u001b[0m        \u001b[32m3.8891\u001b[0m  51.6881\n",
      "      2        \u001b[36m3.8605\u001b[0m        3.8946  52.2106\n",
      "      3        \u001b[36m3.8471\u001b[0m        \u001b[32m3.8779\u001b[0m  51.9858\n",
      "      4        \u001b[36m3.8346\u001b[0m        3.8850  52.6341\n",
      "      5        \u001b[36m3.8258\u001b[0m        3.9279  52.2745\n",
      "      6        \u001b[36m3.8169\u001b[0m        3.8988  52.0407\n",
      "      7        \u001b[36m3.8114\u001b[0m        3.8891  52.1656\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8921\u001b[0m        \u001b[32m3.9673\u001b[0m  51.1147\n",
      "      2        \u001b[36m3.8650\u001b[0m        \u001b[32m3.9460\u001b[0m  52.2445\n",
      "      3        \u001b[36m3.8519\u001b[0m        \u001b[32m3.9310\u001b[0m  51.8529\n",
      "      4        \u001b[36m3.8456\u001b[0m        3.9530  51.5443\n",
      "      5        \u001b[36m3.8358\u001b[0m        3.9532  52.4204\n",
      "      6        \u001b[36m3.8283\u001b[0m        3.9578  51.7870\n",
      "      7        \u001b[36m3.8169\u001b[0m        3.9568  51.8939\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8908\u001b[0m        \u001b[32m3.9356\u001b[0m  51.7990\n",
      "      2        \u001b[36m3.8797\u001b[0m        3.9650  52.2865\n",
      "      3        \u001b[36m3.8612\u001b[0m        \u001b[32m3.9335\u001b[0m  51.4464\n",
      "      4        \u001b[36m3.8489\u001b[0m        3.9413  52.2276\n",
      "      5        \u001b[36m3.8384\u001b[0m        3.9437  52.3035\n",
      "      6        \u001b[36m3.8322\u001b[0m        3.9391  51.9199\n",
      "      7        \u001b[36m3.8268\u001b[0m        \u001b[32m3.9307\u001b[0m  52.2835\n",
      "      8        \u001b[36m3.8228\u001b[0m        3.9329  51.7780\n",
      "      9        \u001b[36m3.8141\u001b[0m        3.9311  52.0867\n",
      "     10        \u001b[36m3.8062\u001b[0m        3.9308  51.9119\n",
      "     11        \u001b[36m3.7995\u001b[0m        \u001b[32m3.9246\u001b[0m  52.2675\n",
      "     12        \u001b[36m3.7941\u001b[0m        3.9302  52.0268\n",
      "     13        \u001b[36m3.7905\u001b[0m        3.9374  52.0248\n",
      "     14        \u001b[36m3.7874\u001b[0m        3.9548  52.0997\n",
      "     15        \u001b[36m3.7827\u001b[0m        3.9570  52.3964\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8877\u001b[0m        \u001b[32m3.9336\u001b[0m  51.1057\n",
      "      2        \u001b[36m3.8661\u001b[0m        3.9446  51.6072\n",
      "      3        \u001b[36m3.8479\u001b[0m        \u001b[32m3.9319\u001b[0m  51.8150\n",
      "      4        \u001b[36m3.8356\u001b[0m        \u001b[32m3.9292\u001b[0m  52.1516\n",
      "      5        \u001b[36m3.8285\u001b[0m        3.9398  52.2276\n",
      "      6        \u001b[36m3.8284\u001b[0m        3.9368  52.3934\n",
      "      7        \u001b[36m3.8152\u001b[0m        3.9629  51.6022\n",
      "      8        \u001b[36m3.8076\u001b[0m        3.9679  52.4313\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8754\u001b[0m        \u001b[32m3.9692\u001b[0m  51.8050\n",
      "      2        \u001b[36m3.8550\u001b[0m        \u001b[32m3.9562\u001b[0m  51.9998\n",
      "      3        \u001b[36m3.8410\u001b[0m        \u001b[32m3.9489\u001b[0m  52.2735\n",
      "      4        \u001b[36m3.8307\u001b[0m        3.9695  51.9878\n",
      "      5        \u001b[36m3.8205\u001b[0m        3.9833  52.6281\n",
      "      6        \u001b[36m3.8138\u001b[0m        3.9577  52.4553\n",
      "      7        \u001b[36m3.8048\u001b[0m        3.9580  54.3823\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8055\u001b[0m        \u001b[32m3.8343\u001b[0m  52.2795\n",
      "      2        \u001b[36m3.7964\u001b[0m        3.8470  51.7590\n",
      "      3        \u001b[36m3.7893\u001b[0m        3.8548  52.3464\n",
      "      4        \u001b[36m3.7817\u001b[0m        3.8629  51.6132\n",
      "      5        \u001b[36m3.7763\u001b[0m        3.8647  52.6611\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8175\u001b[0m        \u001b[32m3.9101\u001b[0m  51.2376\n",
      "      2        \u001b[36m3.8056\u001b[0m        3.9106  51.9339\n",
      "      3        \u001b[36m3.7999\u001b[0m        \u001b[32m3.9062\u001b[0m  51.6721\n",
      "      4        \u001b[36m3.7944\u001b[0m        3.9195  52.2335\n",
      "      5        \u001b[36m3.7872\u001b[0m        3.9336  52.2315\n",
      "      6        \u001b[36m3.7814\u001b[0m        3.9478  51.9259\n",
      "      7        \u001b[36m3.7778\u001b[0m        3.9136  52.3045\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8249\u001b[0m        \u001b[32m3.9166\u001b[0m  51.6921\n",
      "      2        \u001b[36m3.8142\u001b[0m        3.9193  51.8909\n",
      "      3        \u001b[36m3.8058\u001b[0m        3.9248  51.5043\n",
      "      4        \u001b[36m3.8002\u001b[0m        3.9324  51.7081\n",
      "      5        \u001b[36m3.7966\u001b[0m        3.9224  52.1516\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8183\u001b[0m        \u001b[32m3.9245\u001b[0m  50.8559\n",
      "      2        \u001b[36m3.8052\u001b[0m        \u001b[32m3.9194\u001b[0m  51.5742\n",
      "      3        \u001b[36m3.7975\u001b[0m        3.9236  51.1117\n",
      "      4        \u001b[36m3.7893\u001b[0m        3.9204  51.6461\n",
      "      5        \u001b[36m3.7847\u001b[0m        3.9269  51.4324\n",
      "      6        \u001b[36m3.7808\u001b[0m        3.9288  52.0018\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8105\u001b[0m        \u001b[32m3.9169\u001b[0m  50.6811\n",
      "      2        \u001b[36m3.7975\u001b[0m        3.9268  52.0607\n",
      "      3        \u001b[36m3.7882\u001b[0m        3.9266  51.7411\n",
      "      4        \u001b[36m3.7821\u001b[0m        3.9275  51.7740\n",
      "      5        \u001b[36m3.7786\u001b[0m        3.9327  55.1819\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8623\u001b[0m        \u001b[32m3.9001\u001b[0m  50.3695\n",
      "      2        \u001b[36m3.8439\u001b[0m        \u001b[32m3.8979\u001b[0m  51.3345\n",
      "      3        \u001b[36m3.8296\u001b[0m        \u001b[32m3.8976\u001b[0m  51.3205\n",
      "      4        \u001b[36m3.8206\u001b[0m        \u001b[32m3.8923\u001b[0m  51.5782\n",
      "      5        \u001b[36m3.8132\u001b[0m        \u001b[32m3.8753\u001b[0m  50.9279\n",
      "      6        \u001b[36m3.8066\u001b[0m        3.8842  51.3075\n",
      "      7        \u001b[36m3.8019\u001b[0m        3.8863  52.6661\n",
      "      8        \u001b[36m3.7938\u001b[0m        3.8878  51.2805\n",
      "      9        \u001b[36m3.7887\u001b[0m        3.8838  51.8639\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8664\u001b[0m        \u001b[32m3.9265\u001b[0m  50.9059\n",
      "      2        \u001b[36m3.8498\u001b[0m        3.9286  51.4843\n",
      "      3        \u001b[36m3.8384\u001b[0m        3.9268  51.2226\n",
      "      4        \u001b[36m3.8271\u001b[0m        \u001b[32m3.9177\u001b[0m  51.4623\n",
      "      5        \u001b[36m3.8192\u001b[0m        3.9330  51.4174\n",
      "      6        \u001b[36m3.8146\u001b[0m        3.9185  51.1566\n",
      "      7        \u001b[36m3.8084\u001b[0m        3.9215  51.4124\n",
      "      8        \u001b[36m3.8016\u001b[0m        \u001b[32m3.9116\u001b[0m  51.6991\n",
      "      9        \u001b[36m3.7950\u001b[0m        3.9329  51.1836\n",
      "     10        \u001b[36m3.7929\u001b[0m        3.9289  51.7990\n",
      "     11        \u001b[36m3.7862\u001b[0m        3.9446  51.6282\n",
      "     12        \u001b[36m3.7845\u001b[0m        3.9542  50.8639\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8827\u001b[0m        \u001b[32m3.9330\u001b[0m  51.0867\n",
      "      2        \u001b[36m3.8617\u001b[0m        \u001b[32m3.9141\u001b[0m  50.7061\n",
      "      3        \u001b[36m3.8471\u001b[0m        3.9382  51.0887\n",
      "      4        \u001b[36m3.8388\u001b[0m        3.9602  51.8919\n",
      "      5        \u001b[36m3.8309\u001b[0m        3.9484  51.2176\n",
      "      6        \u001b[36m3.8245\u001b[0m        3.9441  51.1117\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8722\u001b[0m        \u001b[32m3.9297\u001b[0m  50.3804\n",
      "      2        \u001b[36m3.8509\u001b[0m        \u001b[32m3.9171\u001b[0m  51.0058\n",
      "      3        \u001b[36m3.8343\u001b[0m        3.9393  51.3784\n",
      "      4        \u001b[36m3.8269\u001b[0m        3.9637  51.0078\n",
      "      5        \u001b[36m3.8206\u001b[0m        3.9473  51.4334\n",
      "      6        \u001b[36m3.8115\u001b[0m        3.9354  51.6771\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8625\u001b[0m        \u001b[32m3.9434\u001b[0m  50.8759\n",
      "      2        \u001b[36m3.8414\u001b[0m        3.9604  50.8050\n",
      "      3        \u001b[36m3.8303\u001b[0m        \u001b[32m3.9162\u001b[0m  51.2016\n",
      "      4        \u001b[36m3.8173\u001b[0m        3.9243  51.4284\n",
      "      5        \u001b[36m3.8097\u001b[0m        3.9525  51.9019\n",
      "      6        \u001b[36m3.8046\u001b[0m        3.9317  51.7700\n",
      "      7        \u001b[36m3.7973\u001b[0m        3.9522  52.2405\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1       \u001b[36m17.5371\u001b[0m        \u001b[32m4.3027\u001b[0m  50.3635\n",
      "      2        \u001b[36m4.9201\u001b[0m        4.3027  51.1227\n",
      "      3        4.9201        4.3027  50.9039\n",
      "      4        4.9201        4.3027  50.8370\n",
      "      5        4.9201        4.3027  50.2476\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.9059\u001b[0m        \u001b[32m4.3201\u001b[0m  50.0947\n",
      "      2        4.9168        4.3201  51.9878\n",
      "      3        4.9168        4.3201  50.9289\n",
      "      4        4.9168        4.3201  51.2536\n",
      "      5        4.9168        4.3201  51.0757\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.9844\u001b[0m        \u001b[32m4.3201\u001b[0m  50.3175\n",
      "      2        4.9947        4.3201  50.6482\n",
      "      3        4.9947        4.3201  50.8629\n",
      "      4        4.9947        4.3201  51.0058\n",
      "      5        4.9947        4.3201  51.0568\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.9671\u001b[0m        \u001b[32m4.3200\u001b[0m  49.4854\n",
      "      2        4.9774        4.3200  50.9708\n",
      "      3        4.9774        4.3200  51.1926\n",
      "      4        4.9774        4.3200  50.6142\n",
      "      5        4.9774        4.3200  51.5732\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m5.0055\u001b[0m        \u001b[32m4.2694\u001b[0m  50.2326\n",
      "      2        5.0169        4.2694  51.0907\n",
      "      3        5.0169        4.2694  51.3654\n",
      "      4        5.0169        4.2694  51.0737\n",
      "      5        5.0169        4.2694  49.8290\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7711\u001b[0m        \u001b[32m3.8137\u001b[0m  50.6901\n",
      "      2        \u001b[36m3.7645\u001b[0m        \u001b[32m3.8102\u001b[0m  50.7930\n",
      "      3        \u001b[36m3.7594\u001b[0m        3.8144  50.9808\n",
      "      4        \u001b[36m3.7554\u001b[0m        3.8156  50.6602\n",
      "      5        \u001b[36m3.7520\u001b[0m        3.8176  50.7561\n",
      "      6        \u001b[36m3.7490\u001b[0m        3.8180  50.8270\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7828\u001b[0m        \u001b[32m3.9094\u001b[0m  51.0248\n",
      "      2        \u001b[36m3.7761\u001b[0m        \u001b[32m3.9087\u001b[0m  50.6691\n",
      "      3        \u001b[36m3.7712\u001b[0m        3.9103  50.5802\n",
      "      4        \u001b[36m3.7673\u001b[0m        \u001b[32m3.9080\u001b[0m  50.6761\n",
      "      5        \u001b[36m3.7637\u001b[0m        \u001b[32m3.9070\u001b[0m  50.6252\n",
      "      6        \u001b[36m3.7604\u001b[0m        3.9087  50.8659\n",
      "      7        \u001b[36m3.7573\u001b[0m        3.9076  51.2396\n",
      "      8        \u001b[36m3.7543\u001b[0m        3.9090  50.8510\n",
      "      9        \u001b[36m3.7517\u001b[0m        3.9086  51.1806\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7923\u001b[0m        \u001b[32m3.9072\u001b[0m  50.1726\n",
      "      2        \u001b[36m3.7857\u001b[0m        \u001b[32m3.9066\u001b[0m  50.7251\n",
      "      3        \u001b[36m3.7811\u001b[0m        3.9085  51.0677\n",
      "      4        \u001b[36m3.7772\u001b[0m        3.9080  50.9589\n",
      "      5        \u001b[36m3.7731\u001b[0m        3.9076  51.0488\n",
      "      6        \u001b[36m3.7701\u001b[0m        3.9085  50.9638\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7849\u001b[0m        \u001b[32m3.9103\u001b[0m  50.6631\n",
      "      2        \u001b[36m3.7782\u001b[0m        \u001b[32m3.9081\u001b[0m  51.1037\n",
      "      3        \u001b[36m3.7733\u001b[0m        3.9097  51.0617\n",
      "      4        \u001b[36m3.7689\u001b[0m        3.9105  50.9229\n",
      "      5        \u001b[36m3.7651\u001b[0m        3.9099  51.7111\n",
      "      6        \u001b[36m3.7616\u001b[0m        3.9117  51.7011\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7792\u001b[0m        \u001b[32m3.9151\u001b[0m  50.6012\n",
      "      2        \u001b[36m3.7717\u001b[0m        3.9168  51.1047\n",
      "      3        \u001b[36m3.7663\u001b[0m        3.9183  51.5862\n",
      "      4        \u001b[36m3.7622\u001b[0m        3.9199  51.2346\n",
      "      5        \u001b[36m3.7582\u001b[0m        3.9206  51.5253\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.9819\u001b[0m        \u001b[32m4.2456\u001b[0m  49.8370\n",
      "      2        \u001b[36m4.7454\u001b[0m        4.2456  51.4653\n",
      "      3        4.7454        4.2456  50.6032\n",
      "      4        4.7454        4.2456  50.8200\n",
      "      5        4.7454        4.2456  50.7531\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.7378\u001b[0m        \u001b[32m4.2602\u001b[0m  51.0018\n",
      "      2        \u001b[36m4.7356\u001b[0m        4.2602  50.9938\n",
      "      3        4.7356        4.2602  51.0128\n",
      "      4        4.7356        4.2602  50.4833\n",
      "      5        4.7356        4.2602  50.4903\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.7002\u001b[0m        \u001b[32m4.2602\u001b[0m  50.5463\n",
      "      2        \u001b[36m4.6980\u001b[0m        4.2602  50.6981\n",
      "      3        4.6980        4.2602  51.0997\n",
      "      4        4.6980        4.2602  50.6711\n",
      "      5        4.6980        4.2602  50.3964\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.6254\u001b[0m        \u001b[32m4.2602\u001b[0m  50.5912\n",
      "      2        \u001b[36m4.6232\u001b[0m        4.2602  50.8639\n",
      "      3        4.6232        4.2602  50.1527\n",
      "      4        4.6232        4.2602  50.3545\n",
      "      5        4.6232        4.2602  50.6761\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.6398\u001b[0m        \u001b[32m4.2445\u001b[0m  50.7730\n",
      "      2        4.6405        4.2445  50.4124\n",
      "      3        4.6405        4.2445  50.7431\n",
      "      4        4.6405        4.2445  51.3275\n",
      "      5        4.6405        4.2445  50.9169\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.7701\u001b[0m        \u001b[32m4.3857\u001b[0m  50.1017\n",
      "      2        \u001b[36m4.7673\u001b[0m        4.3857  50.7111\n",
      "      3        4.7673        4.3857  51.1277\n",
      "      4        4.7673        4.3857  51.7231\n",
      "      5        4.7673        4.3857  50.3974\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1     \u001b[36m7594.1392\u001b[0m        \u001b[32m4.4055\u001b[0m  50.8689\n",
      "      2        \u001b[36m4.7568\u001b[0m        4.4055  50.9768\n",
      "      3        4.7568        4.4055  50.7251\n",
      "      4        4.7568        4.4055  50.5523\n",
      "      5        4.7568        4.4055  50.8829\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1     \u001b[36m7594.1049\u001b[0m        \u001b[32m4.4055\u001b[0m  50.8799\n",
      "      2        \u001b[36m4.7344\u001b[0m        4.4055  50.9569\n",
      "      3        4.7344        4.4055  50.6751\n",
      "      4        4.7344        4.4055  50.8939\n",
      "      5        4.7344        4.4055  51.2296\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1     \u001b[36m7594.0352\u001b[0m        \u001b[32m4.4055\u001b[0m  50.5682\n",
      "      2        \u001b[36m4.6647\u001b[0m        4.4055  50.6971\n",
      "      3        4.6647        4.4055  51.2346\n",
      "      4        4.6647        4.4055  50.6292\n",
      "      5        4.6647        4.4055  50.7730\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1     \u001b[36m7594.0442\u001b[0m        \u001b[32m4.6369\u001b[0m  50.5403\n",
      "      2        \u001b[36m4.6769\u001b[0m        4.6369  50.8669\n",
      "      3        4.6769        4.6369  50.9708\n",
      "      4        4.6769        4.6369  50.7151\n",
      "      5        4.6769        4.6369  50.5483\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7730\u001b[0m        \u001b[32m3.8070\u001b[0m  51.4503\n",
      "      2        \u001b[36m3.7660\u001b[0m        3.8091  51.3125\n",
      "      3        \u001b[36m3.7611\u001b[0m        3.8114  51.4533\n",
      "      4        \u001b[36m3.7572\u001b[0m        3.8152  51.1557\n",
      "      5        \u001b[36m3.7538\u001b[0m        3.8159  50.6931\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7847\u001b[0m        \u001b[32m3.9032\u001b[0m  51.0138\n",
      "      2        \u001b[36m3.7776\u001b[0m        \u001b[32m3.9024\u001b[0m  50.4354\n",
      "      3        \u001b[36m3.7725\u001b[0m        3.9024  51.4174\n",
      "      4        \u001b[36m3.7691\u001b[0m        3.9034  50.4194\n",
      "      5        \u001b[36m3.7651\u001b[0m        3.9037  50.7261\n",
      "      6        \u001b[36m3.7616\u001b[0m        3.9030  50.4014\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7942\u001b[0m        \u001b[32m3.9036\u001b[0m  50.4284\n",
      "      2        \u001b[36m3.7870\u001b[0m        3.9052  50.1727\n",
      "      3        \u001b[36m3.7821\u001b[0m        \u001b[32m3.9018\u001b[0m  50.6042\n",
      "      4        \u001b[36m3.7781\u001b[0m        3.9038  50.7820\n",
      "      5        \u001b[36m3.7742\u001b[0m        3.9056  50.8350\n",
      "      6        \u001b[36m3.7707\u001b[0m        3.9070  50.4534\n",
      "      7        \u001b[36m3.7673\u001b[0m        3.9078  50.9069\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7866\u001b[0m        \u001b[32m3.9063\u001b[0m  50.2526\n",
      "      2        \u001b[36m3.7792\u001b[0m        \u001b[32m3.9059\u001b[0m  50.5872\n",
      "      3        \u001b[36m3.7742\u001b[0m        3.9060  50.4054\n",
      "      4        \u001b[36m3.7696\u001b[0m        3.9079  50.9199\n",
      "      5        \u001b[36m3.7658\u001b[0m        3.9102  50.4264\n",
      "      6        \u001b[36m3.7624\u001b[0m        3.9104  50.3884\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7809\u001b[0m        \u001b[32m3.9197\u001b[0m  50.2935\n",
      "      2        \u001b[36m3.7727\u001b[0m        \u001b[32m3.9193\u001b[0m  50.5323\n",
      "      3        \u001b[36m3.7676\u001b[0m        3.9195  50.5013\n",
      "      4        \u001b[36m3.7632\u001b[0m        \u001b[32m3.9189\u001b[0m  51.1067\n",
      "      5        \u001b[36m3.7596\u001b[0m        3.9203  50.5453\n",
      "      6        \u001b[36m3.7560\u001b[0m        3.9196  50.6901\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.2468\u001b[0m        \u001b[32m4.2295\u001b[0m  50.6212\n",
      "      2        4.3013        4.2295  51.8350\n",
      "      3        4.3013        4.2295  52.1626\n",
      "      4        4.3013        4.2295  51.9488\n",
      "      5        4.3013        4.2295  51.9378\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3006\u001b[0m        \u001b[32m4.2423\u001b[0m  51.2186\n",
      "      2        4.3078        4.2423  51.7311\n",
      "      3        4.3078        4.2423  51.0718\n",
      "      4        4.3078        4.2423  51.2775\n",
      "      5        4.3078        4.2423  51.3794\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3078\u001b[0m        \u001b[32m4.2423\u001b[0m  50.6641\n",
      "      2        4.3150        4.2423  50.9349\n",
      "      3        4.3150        4.2423  51.1277\n",
      "      4        4.3150        4.2423  51.2585\n",
      "      5        4.3150        4.2423  51.0787\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3004\u001b[0m        \u001b[32m4.2423\u001b[0m  50.7960\n",
      "      2        4.3076        4.2423  51.0068\n",
      "      3        4.3076        4.2423  51.5872\n",
      "      4        4.3076        4.2423  51.2116\n",
      "      5        4.3076        4.2423  51.0747\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.3010\u001b[0m        \u001b[32m4.3317\u001b[0m  50.7221\n",
      "      2        4.3081        4.3317  51.3804\n",
      "      3        4.3081        4.3317  51.6012\n",
      "      4        4.3081        4.3317  51.2825\n",
      "      5        4.3081        4.3317  50.9489\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m6.6641\u001b[0m       \u001b[32m11.9000\u001b[0m  51.2535\n",
      "      2        \u001b[36m5.0084\u001b[0m       11.9000  51.2506\n",
      "      3        5.0084       11.9000  51.5722\n",
      "      4        5.0084       11.9000  52.3115\n",
      "      5        5.0084       11.9000  52.2525\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1      \u001b[36m765.7959\u001b[0m       \u001b[32m11.9675\u001b[0m  50.5443\n",
      "      2        \u001b[36m5.0056\u001b[0m       11.9675  50.4114\n",
      "      3        5.0056       11.9675  52.7660\n",
      "      4        5.0056       11.9675  53.6551\n",
      "      5        5.0056       11.9675  54.6161\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1      \u001b[36m765.8180\u001b[0m       \u001b[32m11.9675\u001b[0m  54.8188\n",
      "      2        \u001b[36m5.0981\u001b[0m       11.9675  58.1818\n",
      "      3        5.0981       11.9675  58.7618\n",
      "      4        5.0981       11.9675  59.6933\n",
      "      5        5.0981       11.9675  60.1984\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1      \u001b[36m765.7187\u001b[0m       \u001b[32m11.9675\u001b[0m  52.6291\n",
      "      2        \u001b[36m4.9988\u001b[0m       11.9675  54.7935\n",
      "      3        4.9988       11.9675  58.4112\n",
      "      4        4.9988       11.9675  61.7907\n",
      "      5        4.9988       11.9675  57.5859\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1      \u001b[36m765.7069\u001b[0m        \u001b[32m4.2546\u001b[0m  52.1366\n",
      "      2        \u001b[36m4.9837\u001b[0m        4.2546  51.4803\n",
      "      3        4.9837        4.2546  51.7001\n",
      "      4        4.9837        4.2546  51.9748\n",
      "      5        4.9837        4.2546  51.8509\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7725\u001b[0m        \u001b[32m3.9187\u001b[0m  64.0215\n",
      "      2        \u001b[36m3.7670\u001b[0m        3.9200  65.0839\n",
      "      3        \u001b[36m3.7632\u001b[0m        3.9209  65.0295\n",
      "      4        \u001b[36m3.7603\u001b[0m        3.9215  65.1303\n",
      "      5        \u001b[36m3.7575\u001b[0m        3.9219  64.9196\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "RandomizedSearchCV took 42741.39 seconds for 20 candidates parameter settings.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e6b5eaf03aff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n\u001b[0;32m     52\u001b[0m       \" parameter settings.\" % ((time.time() - start), n_iter_search))\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mreport\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'report' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from scipy.stats import loguniform, uniform\n",
    "params = {\n",
    "    'net__lr': [0.001,0.003,0.01,0.03,0.1,0.3,0.6,0.9,1],\n",
    "    'net__max_epochs': [20,40,50],\n",
    "    'net__optimizer__momentum': [0.95,0.96,0.97,0.98,0.99]\n",
    "}\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'net__lr': loguniform(0.001,1),\n",
    "              'net__max_epochs': [20,30,40,50],\n",
    "              'net__optimizer__momentum': loguniform(0.9, 0.99)}\n",
    "\n",
    "pipe = Pipeline([(\"typetransform\", typetransform), (\"net\", net)])\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')\n",
    "x_train2 = tfidf.fit_transform(x_train)\n",
    "x_trainshape = x_train2.shape[1]\n",
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "pole_model = RegressorModule()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(pole_model.parameters(), lr = 0.1)\n",
    "\n",
    "net = NeuralNetRegressor(module=pole_model, max_epochs=30, lr=0.1, callbacks =[('earlystopping',EarlyStopping())])\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time.time()\n",
    "random_search.fit(x_train2, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "\n",
    "# gs = GridSearchCV(pipe, params, n_jobs=-1)\n",
    "# gs.fit(X=x_train, y=y_train);\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "color-northwest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.099 (std: 0.011)\n",
      "Parameters: {'net__lr': 0.0015539382932245303, 'net__max_epochs': 20, 'net__optimizer__momentum': 0.9375124227337543}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.098 (std: 0.011)\n",
      "Parameters: {'net__lr': 0.0016770124951511836, 'net__max_epochs': 20, 'net__optimizer__momentum': 0.9457840905487397}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.097 (std: 0.011)\n",
      "Parameters: {'net__lr': 0.002630441086119309, 'net__max_epochs': 40, 'net__optimizer__momentum': 0.9183456932254296}\n",
      "\n",
      "{'net__lr': 0.0015539382932245303, 'net__max_epochs': 20, 'net__optimizer__momentum': 0.9375124227337543}\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_randomized_search.html\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "\n",
    "report(random_search.cv_results_)\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "intellectual-paris",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8644\u001b[0m        \u001b[32m3.9511\u001b[0m  68.8521\n",
      "      2        \u001b[36m3.8590\u001b[0m        \u001b[32m3.9415\u001b[0m  68.8259\n",
      "      3        \u001b[36m3.8525\u001b[0m        3.9416  68.9574\n",
      "      4        \u001b[36m3.8465\u001b[0m        3.9420  69.1866\n",
      "      5        \u001b[36m3.8413\u001b[0m        \u001b[32m3.9412\u001b[0m  69.2820\n",
      "      6        \u001b[36m3.8359\u001b[0m        \u001b[32m3.9374\u001b[0m  69.7446\n",
      "      7        \u001b[36m3.8313\u001b[0m        \u001b[32m3.9369\u001b[0m  68.9858\n",
      "      8        \u001b[36m3.8265\u001b[0m        3.9419  69.9018\n",
      "      9        \u001b[36m3.8217\u001b[0m        3.9387  71.4834\n",
      "     10        \u001b[36m3.8171\u001b[0m        \u001b[32m3.9361\u001b[0m  73.6768\n",
      "     11        \u001b[36m3.8129\u001b[0m        3.9373  74.3237\n",
      "     12        \u001b[36m3.8091\u001b[0m        3.9397  66.4101\n",
      "     13        \u001b[36m3.8048\u001b[0m        3.9407  65.8983\n",
      "     14        \u001b[36m3.8012\u001b[0m        3.9412  66.0906\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Optimised RMSE: 1.9759188\n",
      "Re-initializing optimizer because the following parameters were re-set: momentum.\n",
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7937\u001b[0m        \u001b[32m3.9409\u001b[0m  65.5602\n",
      "      2        \u001b[36m3.7901\u001b[0m        \u001b[32m3.9341\u001b[0m  65.2272\n",
      "      3        \u001b[36m3.7858\u001b[0m        3.9359  65.7110\n",
      "      4        \u001b[36m3.7823\u001b[0m        3.9355  65.7200\n",
      "      5        \u001b[36m3.7791\u001b[0m        3.9382  65.1797\n",
      "      6        \u001b[36m3.7760\u001b[0m        3.9387  65.9037\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Original RMSE: 1.9748926\n"
     ]
    }
   ],
   "source": [
    "optimum_params = {'net__lr': 0.0015539382932245303, 'net__max_epochs': 20, 'net__optimizer__momentum': 0.9375124227337543}\n",
    "\n",
    "net = NeuralNetRegressor(module=pole_model, max_epochs=optimum_params['net__max_epochs'], lr=optimum_params['net__lr'], optimizer__momentum=optimum_params['net__optimizer__momentum'], callbacks =[('earlystopping',EarlyStopping())])\n",
    "pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"typetransform\", typetransform), (\"net\", net)])\n",
    "\n",
    "pipe.fit(X=x_train, y=y_train)\n",
    "y_pred = pipe.predict(x_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "print('Optimised RMSE:', rmse)\n",
    "\n",
    "net = NeuralNetRegressor(module=pole_model, max_epochs=30, lr=0.1, callbacks =[('earlystopping',EarlyStopping())])\n",
    "\n",
    "pipe.fit(X=x_train, y=y_train)\n",
    "y_pred = pipe.predict(x_test)\n",
    "\n",
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "print('Original RMSE:', rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "single-plastic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8194\u001b[0m        \u001b[32m3.9282\u001b[0m  69.1231\n",
      "      2        \u001b[36m3.8148\u001b[0m        3.9283  68.3899\n",
      "      3        \u001b[36m3.8120\u001b[0m        \u001b[32m3.9277\u001b[0m  69.3362\n",
      "      4        \u001b[36m3.8099\u001b[0m        3.9283  68.5192\n",
      "      5        \u001b[36m3.8078\u001b[0m        3.9283  69.8550\n",
      "      6        \u001b[36m3.8059\u001b[0m        3.9294  69.3251\n",
      "      7        \u001b[36m3.8043\u001b[0m        3.9297  69.3530\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Softmax RMSE: 1.9710398\n",
      "ReLU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.8013\u001b[0m        \u001b[32m3.9297\u001b[0m  64.5240\n",
      "      2        \u001b[36m3.7999\u001b[0m        3.9310  65.2932\n",
      "      3        \u001b[36m3.7986\u001b[0m        3.9304  64.9056\n",
      "      4        \u001b[36m3.7972\u001b[0m        3.9299  65.0265\n",
      "      5        \u001b[36m3.7960\u001b[0m        3.9298  63.9895\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "ReLU RMSE: 1.971134\n",
      "Tanh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7939\u001b[0m        \u001b[32m3.9297\u001b[0m  66.0624\n",
      "      2        \u001b[36m3.7926\u001b[0m        \u001b[32m3.9295\u001b[0m  66.9165\n",
      "      3        \u001b[36m3.7914\u001b[0m        3.9297  66.3971\n",
      "      4        \u001b[36m3.7902\u001b[0m        3.9299  66.9555\n",
      "      5        \u001b[36m3.7890\u001b[0m        3.9308  67.7387\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Tanh RMSE: 1.9713187\n",
      "Sigmoid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m3.7869\u001b[0m        \u001b[32m3.9310\u001b[0m  64.5859\n",
      "      2        \u001b[36m3.7855\u001b[0m        3.9313  65.1004\n",
      "      3        \u001b[36m3.7848\u001b[0m        \u001b[32m3.9307\u001b[0m  65.5130\n",
      "      4        \u001b[36m3.7837\u001b[0m        3.9308  65.4021\n",
      "      5        \u001b[36m3.7828\u001b[0m        \u001b[32m3.9299\u001b[0m  64.1793\n",
      "      6        \u001b[36m3.7819\u001b[0m        3.9312  64.6079\n",
      "      7        \u001b[36m3.7805\u001b[0m        3.9313  65.8117\n",
      "      8        \u001b[36m3.7799\u001b[0m        3.9309  64.4700\n",
      "      9        \u001b[36m3.7786\u001b[0m        3.9310  64.5439\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n",
      "Sigmoid RMSE: 1.9714288\n"
     ]
    }
   ],
   "source": [
    "for act in ['Softmax','ReLU', 'Tanh', 'Sigmoid']:\n",
    "    print(act)\n",
    "    \n",
    "\n",
    "    if act == 'Softmax':\n",
    "        class RegressorModule(nn.Module):\n",
    "            def __init__(\n",
    "                    self,\n",
    "                    num_units=10,\n",
    "                    nonlin=F.relu,\n",
    "            ):\n",
    "                super(RegressorModule, self).__init__()\n",
    "                self.num_units = num_units\n",
    "                self.nonlin = nonlin\n",
    "\n",
    "                self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "                self.nonlin = nonlin\n",
    "                self.dense1 = nn.Linear(num_units, 10)\n",
    "                self.output = nn.Linear(10, 1)\n",
    "\n",
    "            def forward(self, X, **kwargs):\n",
    "                X = self.nonlin(self.dense0(X))\n",
    "                X = F.Softmax(self.dense1(X))\n",
    "                X = self.output(X)\n",
    "                return X\n",
    "        \n",
    "    elif act == 'ReLU':        \n",
    "        class RegressorModule(nn.Module):\n",
    "            def __init__(\n",
    "                    self,\n",
    "                    num_units=10,\n",
    "                    nonlin=F.relu,\n",
    "            ):\n",
    "                super(RegressorModule, self).__init__()\n",
    "                self.num_units = num_units\n",
    "                self.nonlin = nonlin\n",
    "\n",
    "                self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "                self.nonlin = nonlin\n",
    "                self.dense1 = nn.Linear(num_units, 10)\n",
    "                self.output = nn.Linear(10, 1)\n",
    "\n",
    "            def forward(self, X, **kwargs):\n",
    "                X = self.nonlin(self.dense0(X))\n",
    "                X = F.relu(self.dense1(X))\n",
    "                X = self.output(X)\n",
    "                return X\n",
    "\n",
    "    elif act == 'Tanh':         \n",
    "        class RegressorModule(nn.Module):\n",
    "            def __init__(\n",
    "                    self,\n",
    "                    num_units=10,\n",
    "                    nonlin=F.relu,\n",
    "            ):\n",
    "                super(RegressorModule, self).__init__()\n",
    "                self.num_units = num_units\n",
    "                self.nonlin = nonlin\n",
    "\n",
    "                self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "                self.nonlin = nonlin\n",
    "                self.dense1 = nn.Linear(num_units, 10)\n",
    "                self.output = nn.Linear(10, 1)\n",
    "\n",
    "            def forward(self, X, **kwargs):\n",
    "                X = self.nonlin(self.dense0(X))\n",
    "                X = F.Tanh(self.dense1(X))\n",
    "                X = self.output(X)\n",
    "                return X\n",
    "            \n",
    "    elif act == 'Sigmoid':  \n",
    "        class RegressorModule(nn.Module):\n",
    "            def __init__(\n",
    "                    self,\n",
    "                    num_units=10,\n",
    "                    nonlin=F.relu,\n",
    "            ):\n",
    "                super(RegressorModule, self).__init__()\n",
    "                self.num_units = num_units\n",
    "                self.nonlin = nonlin\n",
    "\n",
    "                self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "                self.nonlin = nonlin\n",
    "                self.dense1 = nn.Linear(num_units, 10)\n",
    "                self.output = nn.Linear(10, 1)\n",
    "\n",
    "            def forward(self, X, **kwargs):\n",
    "                X = self.nonlin(self.dense0(X))\n",
    "                X = F.sigmoid(self.dense1(X))\n",
    "                X = self.output(X)\n",
    "                return X\n",
    "    net = NeuralNetRegressor(module=pole_model, max_epochs=optimum_params['net__max_epochs'], lr=optimum_params['net__lr'], optimizer__momentum=optimum_params['net__optimizer__momentum'], callbacks =[('earlystopping',EarlyStopping())])\n",
    "    pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"typetransform\", typetransform), (\"net\", net)])\n",
    "\n",
    "    pipe.fit(X=x_train, y=y_train)\n",
    "    y_pred = pipe.predict(x_test)\n",
    "\n",
    "    rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "    print(act,'RMSE:', rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-facing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.DataFrame(y_test)\n",
    "dftest['index'] = list(range(103427))\n",
    "dfpred = pd.DataFrame(y_pred)\n",
    "dfpred['index'] = list(range(103427))\n",
    "fix, ax = plt.subplots()\n",
    "sns.kdeplot(x=dftest['index'],y=dftest[0], cumulative=True, color='orange', label='real')\n",
    "sns.kdeplot(x=dfpred['index'],y=dfpred[0], cumulative=True, color='b', label='pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-syntax",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "text_classifier = RandomForestRegressor(n_estimators=100, random_state=0)  \n",
    "text_classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "predictions = text_classifier.predict(x_test)\n",
    " \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    " \n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "elegant-consumer",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'get_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b9d3a85ec1ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'get_params'"
     ]
    }
   ],
   "source": [
    "cross_val_score.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "psychological-rotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = merged['Change']\n",
    "\n",
    "X = processed_tweets\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['text'] = processed_tweets\n",
    "data['y'] = y.tolist()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "data1 = data.sample(frac=0.2)\n",
    "X1 = data1['text'].tolist()\n",
    "y2 = np.array(data1['y'])\n",
    "x_trainsvm, x_testsvm, y_trainsvm, y_testsvm = train_test_split(X1, y2, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "exciting-spyware",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  1705.32363986969 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR, SVR\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "# y_trainsvm = np.array(y_train).ravel()\n",
    "# y_testsvm = np.array(y_test).ravel()\n",
    "\n",
    "#Create the SVM model\n",
    "\n",
    "start = time.time()\n",
    "# regressor = LinearSVR(random_state = 0)\n",
    "regressor = SVR()\n",
    "#Fit the model for the data\n",
    "pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"regressor\", regressor)])\n",
    "\n",
    "pipe.fit(x_trainsvm, y_trainsvm)\n",
    "\n",
    "#Make the prediction\n",
    "y_predsvm = pipe.predict(x_testsvm)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed Time: \", (end-start),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eleven-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.977553460848178\n",
      "R-Squared: 0.0333595799537606\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_testsvm, y_predsvm, squared = False)\n",
    "print(\"RMSE:\",rmse)\n",
    "r2 = r2_score(y_testsvm,y_predsvm)\n",
    "print(\"R-Squared:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-nevada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "significant-angle",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = regressor, X = x_trainsvm, y = y_trainsvm, cv = 10)\n",
    "end = time.time()\n",
    "# print(\"Fraction: \", x)\n",
    "# print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "# print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "mse = mean_squared_error(y_test,y_predsvm)\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "r2 = r2_score(y_test,y_predsvm)\n",
    "print(\"R-squared: {}\".format(r2))\n",
    "print(\"\")\n",
    "print(\"Elapsed Time: \", (end-start),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "pressed-myanmar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00098072231255\n",
      "Mean Squared Error: 4.00098072231255\n",
      "R-squared: 0.025518011281076225\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test,y_predsvm)\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "r2 = r2_score(y_test,y_predsvm)\n",
    "print(\"R-squared: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-exchange",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciEnv]",
   "language": "python",
   "name": "conda-env-DataSciEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
