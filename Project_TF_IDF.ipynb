{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "resistant-problem",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import dateutil.parser as parser\n",
    "from datetime import datetime, date, timedelta\n",
    "import torch\n",
    "import skorch\n",
    "import scipy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "from skorch.helper import DataFrameTransformer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from skorch.callbacks import EarlyStopping\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-credit",
   "metadata": {},
   "source": [
    "# Please ignore the below cells, they were for additional preprocessing and do not need to be run again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "radio-logistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening JSON file\n",
    "f = open('twitter_aapl_1.json',)\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "aapl = json.load(f)\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "thrown-natural",
   "metadata": {},
   "outputs": [],
   "source": [
    "flataapl  = [val for sublist in aapl for val in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "portable-calculation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'created_at': '2021-05-11T10:18:35.000Z', 'id': '1392061565185236994', 'text': 'RT @Nicochan33: Apple Execs Chose to Keep a Hack of 128 Million iPhones Quiet https://t.co/waR9tgHKCA #tech #feedly #apple #iphone #cyberse‚Ä¶'}\n"
     ]
    }
   ],
   "source": [
    "print(flataapl[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "familiar-tissue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>withheld</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11T10:18:35.000Z</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11T10:18:30.000Z</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11T10:18:25.000Z</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11T10:17:54.000Z</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11T10:17:48.000Z</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18T07:25:14.000Z</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18T07:25:08.000Z</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18T07:25:06.000Z</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18T07:24:56.000Z</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18T07:24:54.000Z</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      created_at                   id  \\\n",
       "0       2021-05-11T10:18:35.000Z  1392061565185236994   \n",
       "1       2021-05-11T10:18:30.000Z  1392061546751217671   \n",
       "2       2021-05-11T10:18:25.000Z  1392061526614360066   \n",
       "3       2021-05-11T10:17:54.000Z  1392061393415909376   \n",
       "4       2021-05-11T10:17:48.000Z  1392061371886485506   \n",
       "...                          ...                  ...   \n",
       "517126  2021-02-18T07:25:14.000Z  1362302135333969929   \n",
       "517127  2021-02-18T07:25:08.000Z  1362302111027929089   \n",
       "517128  2021-02-18T07:25:06.000Z  1362302102404542467   \n",
       "517129  2021-02-18T07:24:56.000Z  1362302061589708805   \n",
       "517130  2021-02-18T07:24:54.000Z  1362302053889019904   \n",
       "\n",
       "                                                     text withheld  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha...      NaN  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr...      NaN  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia...      NaN  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK...      NaN  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...      NaN  \n",
       "...                                                   ...      ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New...      NaN  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...      NaN  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps...      NaN  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd      NaN  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...      NaN  \n",
       "\n",
       "[517131 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(flataapl)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "norwegian-analysis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152369    {'copyright': False, 'country_codes': ['IN']}\n",
       "257375    {'copyright': False, 'country_codes': ['IN']}\n",
       "269997    {'copyright': False, 'country_codes': ['IN']}\n",
       "315545    {'copyright': False, 'country_codes': ['RU']}\n",
       "315578    {'copyright': False, 'country_codes': ['RU']}\n",
       "343775    {'copyright': False, 'country_codes': ['IN']}\n",
       "473481    {'copyright': False, 'country_codes': ['RU']}\n",
       "473719    {'copyright': False, 'country_codes': ['RU']}\n",
       "489545    {'copyright': False, 'country_codes': ['IN']}\n",
       "509238    {'copyright': False, 'country_codes': ['IN']}\n",
       "Name: withheld, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['withheld'].dropna()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "handled-satisfaction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11T10:18:35</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11T10:18:30</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11T10:18:25</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11T10:17:54</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11T10:17:48</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18T07:25:14</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18T07:25:08</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18T07:25:06</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18T07:24:56</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18T07:24:54</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id  \\\n",
       "0       2021-05-11T10:18:35  1392061565185236994   \n",
       "1       2021-05-11T10:18:30  1392061546751217671   \n",
       "2       2021-05-11T10:18:25  1392061526614360066   \n",
       "3       2021-05-11T10:17:54  1392061393415909376   \n",
       "4       2021-05-11T10:17:48  1392061371886485506   \n",
       "...                     ...                  ...   \n",
       "517126  2021-02-18T07:25:14  1362302135333969929   \n",
       "517127  2021-02-18T07:25:08  1362302111027929089   \n",
       "517128  2021-02-18T07:25:06  1362302102404542467   \n",
       "517129  2021-02-18T07:24:56  1362302061589708805   \n",
       "517130  2021-02-18T07:24:54  1362302053889019904   \n",
       "\n",
       "                                                     text  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha...  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr...  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia...  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK...  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...  \n",
       "...                                                   ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New...  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps...  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...  \n",
       "\n",
       "[517131 rows x 3 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = df.drop('withheld',axis=1)\n",
    "df1['created_at'] = df1['created_at'].map(lambda x: str(x)[:-5])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "registered-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv(\"twitter_aapl_1_clean.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pacific-homework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Date                   id  \\\n",
       "0       2021-05-11  1392061565185236994   \n",
       "1       2021-05-11  1392061546751217671   \n",
       "2       2021-05-11  1392061526614360066   \n",
       "3       2021-05-11  1392061393415909376   \n",
       "4       2021-05-11  1392061371886485506   \n",
       "...            ...                  ...   \n",
       "517126  2021-02-18  1362302135333969929   \n",
       "517127  2021-02-18  1362302111027929089   \n",
       "517128  2021-02-18  1362302102404542467   \n",
       "517129  2021-02-18  1362302061589708805   \n",
       "517130  2021-02-18  1362302053889019904   \n",
       "\n",
       "                                                     text  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha...  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr...  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia...  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK...  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...  \n",
       "...                                                   ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New...  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps...  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...  \n",
       "\n",
       "[517131 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"twitter_aapl_1_clean.csv\")\n",
    "# Only including this step for now, basically removes the time component, leaving only date\n",
    "df1['created_at'] = df1['created_at'].map(lambda x: str(x)[:-9])\n",
    "df1 = df1.rename(columns={\"created_at\" : \"Date\"})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tough-spine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-14\n",
      "<class 'datetime.date'>\n"
     ]
    }
   ],
   "source": [
    "# adapted from https://www.30secondsofcode.org/python/s/is-weekend\n",
    "def if_weekend(d = datetime.today()):\n",
    "    if d.weekday() == 5:\n",
    "        da = d + timedelta(days=2)\n",
    "        return da\n",
    "    elif d.weekday() == 6:\n",
    "        da = d + timedelta(days=1)\n",
    "        return da\n",
    "    elif d == date(2021,4,2):\n",
    "        return date(2021,4,5)\n",
    "    else:\n",
    "        return d\n",
    "print(if_weekend(date(2021,6,14)))\n",
    "print(type(date(2021,6,14)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sought-there",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>usedate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "      <td>2021-05-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "      <td>2021-02-18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                   id  \\\n",
       "0      2021-05-11  1392061565185236994   \n",
       "1      2021-05-11  1392061546751217671   \n",
       "2      2021-05-11  1392061526614360066   \n",
       "3      2021-05-11  1392061393415909376   \n",
       "4      2021-05-11  1392061371886485506   \n",
       "...           ...                  ...   \n",
       "517126 2021-02-18  1362302135333969929   \n",
       "517127 2021-02-18  1362302111027929089   \n",
       "517128 2021-02-18  1362302102404542467   \n",
       "517129 2021-02-18  1362302061589708805   \n",
       "517130 2021-02-18  1362302053889019904   \n",
       "\n",
       "                                                     text    usedate  \n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha... 2021-05-11  \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr... 2021-05-11  \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia... 2021-05-11  \n",
       "3       RT @iTech911: Future versions of #Apple's CarK... 2021-05-11  \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #... 2021-05-11  \n",
       "...                                                   ...        ...  \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New... 2021-02-18  \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ... 2021-02-18  \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps... 2021-02-18  \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd 2021-02-18  \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact... 2021-02-18  \n",
       "\n",
       "[517131 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Date']= pd.to_datetime(df1['Date'])\n",
    "df1['usedate'] = df1['Date'].map(lambda x: if_weekend(x))\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "concerned-speech",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usedate</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.9100</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5000</td>\n",
       "      <td>126.2700</td>\n",
       "      <td>122.7700</td>\n",
       "      <td>2.4100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-10</td>\n",
       "      <td>126.8500</td>\n",
       "      <td>88071230</td>\n",
       "      <td>129.4100</td>\n",
       "      <td>129.5400</td>\n",
       "      <td>126.8100</td>\n",
       "      <td>-2.5600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-07</td>\n",
       "      <td>130.2100</td>\n",
       "      <td>78973270</td>\n",
       "      <td>130.8500</td>\n",
       "      <td>131.2582</td>\n",
       "      <td>129.4750</td>\n",
       "      <td>-0.6400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-06</td>\n",
       "      <td>129.7400</td>\n",
       "      <td>78128330</td>\n",
       "      <td>127.8900</td>\n",
       "      <td>129.7500</td>\n",
       "      <td>127.1300</td>\n",
       "      <td>1.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-05</td>\n",
       "      <td>128.1000</td>\n",
       "      <td>84000900</td>\n",
       "      <td>129.2000</td>\n",
       "      <td>130.4500</td>\n",
       "      <td>127.9700</td>\n",
       "      <td>-1.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>78.7400</td>\n",
       "      <td>135372520</td>\n",
       "      <td>78.2925</td>\n",
       "      <td>79.1250</td>\n",
       "      <td>77.5810</td>\n",
       "      <td>0.4475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>2020-05-15</td>\n",
       "      <td>76.9275</td>\n",
       "      <td>166348360</td>\n",
       "      <td>75.0875</td>\n",
       "      <td>76.9750</td>\n",
       "      <td>75.0525</td>\n",
       "      <td>1.8400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>2020-05-14</td>\n",
       "      <td>77.3850</td>\n",
       "      <td>158929080</td>\n",
       "      <td>76.1275</td>\n",
       "      <td>77.4475</td>\n",
       "      <td>75.3825</td>\n",
       "      <td>1.2575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>2020-05-13</td>\n",
       "      <td>76.9125</td>\n",
       "      <td>200622560</td>\n",
       "      <td>78.0375</td>\n",
       "      <td>78.9875</td>\n",
       "      <td>75.8025</td>\n",
       "      <td>-1.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>2020-05-12</td>\n",
       "      <td>77.8525</td>\n",
       "      <td>162301040</td>\n",
       "      <td>79.4575</td>\n",
       "      <td>79.9220</td>\n",
       "      <td>77.7275</td>\n",
       "      <td>-1.6050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       usedate  Close/Last     Volume      Open      High       Low  Change\n",
       "0   2021-05-11    125.9100  126142800  123.5000  126.2700  122.7700  2.4100\n",
       "1   2021-05-10    126.8500   88071230  129.4100  129.5400  126.8100 -2.5600\n",
       "2   2021-05-07    130.2100   78973270  130.8500  131.2582  129.4750 -0.6400\n",
       "3   2021-05-06    129.7400   78128330  127.8900  129.7500  127.1300  1.8500\n",
       "4   2021-05-05    128.1000   84000900  129.2000  130.4500  127.9700 -1.1000\n",
       "..         ...         ...        ...       ...       ...       ...     ...\n",
       "247 2020-05-18     78.7400  135372520   78.2925   79.1250   77.5810  0.4475\n",
       "248 2020-05-15     76.9275  166348360   75.0875   76.9750   75.0525  1.8400\n",
       "249 2020-05-14     77.3850  158929080   76.1275   77.4475   75.3825  1.2575\n",
       "250 2020-05-13     76.9125  200622560   78.0375   78.9875   75.8025 -1.1250\n",
       "251 2020-05-12     77.8525  162301040   79.4575   79.9220   77.7275 -1.6050\n",
       "\n",
       "[252 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = pd.read_csv(\"aapl2.csv\")\n",
    "price['Date'] = price['Date'].map(lambda x: parser.parse(x).isoformat())\n",
    "# Only including this step for now, basically removes the time component, leaving only date\n",
    "price['Date'] = price['Date'].map(lambda x: str(x)[:-9])\n",
    "price['Date']= pd.to_datetime(price['Date'])\n",
    "# Change me if looking to change examination scope\n",
    "price = price.rename(columns={\"Date\" : \"usedate\"})\n",
    "price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "headed-flesh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>usedate</th>\n",
       "      <th>Close/Last</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061565185236994</td>\n",
       "      <td>RT @Nicochan33: Apple Execs Chose to Keep a Ha...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061546751217671</td>\n",
       "      <td>RT @RoseLoveStyle: House of the Dragon, the pr...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061526614360066</td>\n",
       "      <td>RT @gtorges: Ich habe jetzt einiges an Materia...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061393415909376</td>\n",
       "      <td>RT @iTech911: Future versions of #Apple's CarK...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>1392061371886485506</td>\n",
       "      <td>Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #...</td>\n",
       "      <td>2021-05-11</td>\n",
       "      <td>125.91</td>\n",
       "      <td>126142800</td>\n",
       "      <td>123.5</td>\n",
       "      <td>126.270</td>\n",
       "      <td>122.77</td>\n",
       "      <td>2.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302135333969929</td>\n",
       "      <td>RT @mobitrade_: Original Apple Airpods Pro New...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302111027929089</td>\n",
       "      <td>#ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302102404542467</td>\n",
       "      <td>Sommige appels worden te snel slecht.\\n\\nhttps...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302061589708805</td>\n",
       "      <td>#apple #kia #StockMarkets https://t.co/ccRBBAKDtd</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>1362302053889019904</td>\n",
       "      <td>RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact...</td>\n",
       "      <td>2021-02-18</td>\n",
       "      <td>129.71</td>\n",
       "      <td>96856750</td>\n",
       "      <td>129.2</td>\n",
       "      <td>129.995</td>\n",
       "      <td>127.41</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date                   id  \\\n",
       "0      2021-05-11  1392061565185236994   \n",
       "1      2021-05-11  1392061546751217671   \n",
       "2      2021-05-11  1392061526614360066   \n",
       "3      2021-05-11  1392061393415909376   \n",
       "4      2021-05-11  1392061371886485506   \n",
       "...           ...                  ...   \n",
       "517126 2021-02-18  1362302135333969929   \n",
       "517127 2021-02-18  1362302111027929089   \n",
       "517128 2021-02-18  1362302102404542467   \n",
       "517129 2021-02-18  1362302061589708805   \n",
       "517130 2021-02-18  1362302053889019904   \n",
       "\n",
       "                                                     text    usedate  \\\n",
       "0       RT @Nicochan33: Apple Execs Chose to Keep a Ha... 2021-05-11   \n",
       "1       RT @RoseLoveStyle: House of the Dragon, the pr... 2021-05-11   \n",
       "2       RT @gtorges: Ich habe jetzt einiges an Materia... 2021-05-11   \n",
       "3       RT @iTech911: Future versions of #Apple's CarK... 2021-05-11   \n",
       "4       Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åüü§ó\\n#Apple #airtag #„Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ #... 2021-05-11   \n",
       "...                                                   ...        ...   \n",
       "517126  RT @mobitrade_: Original Apple Airpods Pro New... 2021-02-18   \n",
       "517127  #ios #Apple #swiftui \\nSwiftUI 2.0 Complex UI ... 2021-02-18   \n",
       "517128  Sommige appels worden te snel slecht.\\n\\nhttps... 2021-02-18   \n",
       "517129  #apple #kia #StockMarkets https://t.co/ccRBBAKDtd 2021-02-18   \n",
       "517130  RT @mobitrade_: iPhone 8 Neatly Used 64GB Fact... 2021-02-18   \n",
       "\n",
       "        Close/Last     Volume   Open     High     Low  Change  \n",
       "0           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "1           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "2           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "3           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "4           125.91  126142800  123.5  126.270  122.77    2.41  \n",
       "...            ...        ...    ...      ...     ...     ...  \n",
       "517126      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517127      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517128      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517129      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "517130      129.71   96856750  129.2  129.995  127.41    0.51  \n",
       "\n",
       "[517131 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.merge(df1, price, how='left', on='usedate')\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "civilian-breathing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date          0\n",
       "id            0\n",
       "text          0\n",
       "usedate       0\n",
       "Close/Last    0\n",
       "Volume        0\n",
       "Open          0\n",
       "High          0\n",
       "Low           0\n",
       "Change        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "therapeutic-favorite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merged.to_csv(\"merged.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-briefing",
   "metadata": {},
   "source": [
    "# Start Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "normal-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         2020 is almost ended\\n\\nBRAND NEW SEALED FACTO...\n",
      "1         If you feel like Apple has no love for you by ...\n",
      "2         üì¢NEW RIP IS LIVEüì¢\\n\\n2W1B 051 - iPhone, Capita...\n",
      "3         #Apple has introduced a new #macOS version of ...\n",
      "4         Good Dealüòç #MacBookPro #AppleEvent #Apple\\n\\nN...\n",
      "                                ...                        \n",
      "508177    Foreign Exchange History: How it All Started?\\...\n",
      "508178    Tropicana Apple Juice, 10 oz., 24 Count\\n\\nSta...\n",
      "508179    \"emotional conclusion to a dazzling series!\"\\n...\n",
      "508180    3ÔºÖË∂Ö‰∏äÊòá„Åó„ÅüÈäòÊüÑ\\n4434  „Çµ„Éº„Éê„Éº„ÉØ„Éº„ÇØ„Çπ\\n4625  „Ç¢„Éà„Éü„ÇØ„Çπ\\n6777  ...\n",
      "508181    EP 42: @seanmagers and @keithrconrad discuss t...\n",
      "Name: text, Length: 508182, dtype: object\n",
      "2020 is almost ended\n",
      "\n",
      "BRAND NEW SEALED FACTORY UNLOCKED iPHONE12 Mini to one LUCKY FOLLOWER.\n",
      "\n",
      "Keep Following &amp; Retweeting.\n",
      "\n",
      "#iTech911 #iBuy #iSell #iSwap #iFix #APPLE Ô£ø\n",
      "\n",
      "0262666226 üí¨ üìû https://t.co/3vOZQIN8yz\n",
      "If you feel like Apple has no love for you by not delivering the Heart Month Challenge badge, just restart your watch and wait for a few minutes. You‚Äôll get it!\n",
      "#apple #AppleWatch #challenge https://t.co/uWuvmVmRcX\n",
      "üì¢NEW RIP IS LIVEüì¢\n",
      "\n",
      "2W1B 051 - iPhone, Capitalism, Amazon Tax Gripes\n",
      "\n",
      "https://t.co/Tm4d6ftR57\n",
      "\n",
      "#Mondaythoughts #Mondaymorning #MondayMotivation #Hamont #BTC #Bitcoin #iphone12 #Apple $AAPL #eeftober #spotify #PS5 #Investing #iTunes #Sony $WEED $CGC #ETFs $AMZN $SPOT #nlpoli https://t.co/ckeMypyOLA\n"
     ]
    }
   ],
   "source": [
    "merged = pd.read_csv('mergedfull.csv')\n",
    "from sklearn.model_selection import train_test_split  \n",
    "mergedbig, mergedsmall = train_test_split(merged, test_size=0.15, random_state=0)\n",
    "mergedsmall.to_csv(\"mergedsmall.csv\", index = False)\n",
    "mergedsmall = pd.read_csv('mergedsmall.csv')\n",
    "merged = mergedsmall\n",
    "# Obtaining the tweet contents into a list\n",
    "all_tweets = merged[\"text\"]\n",
    "print(all_tweets)\n",
    "all_tweets = all_tweets.to_list()\n",
    "print(all_tweets[0])\n",
    "print(all_tweets[1])\n",
    "print(all_tweets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "breeding-booth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "508182\n"
     ]
    }
   ],
   "source": [
    "print(len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "promising-roman",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemming(tweet):\n",
    "    a = word_tokenize(tweet)\n",
    "    answer = list(map(lambda x: lemmatizer.lemmatize(x), a))\n",
    "    return answer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "confident-convert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.gotrained.com/scraping-tweets-sentiment-analysis/\n",
    "# Basic cleaning of text before TF-IDF; this process will be improved later\n",
    "# for tweet in all_tweets:\n",
    "#     # Remove all the special characters\n",
    "#     processed_tweet = re.sub(r'\\W', ' ', tweet)\n",
    " \n",
    "#     # remove all single characters\n",
    "#     processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "#     # Remove single characters from the start\n",
    "#     processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "#     # Substituting multiple spaces with single space\n",
    "#     processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "#     # Removing prefixed 'b'\n",
    "#     processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    "    \n",
    "#     #removing common pronouns and prepositions\n",
    "#     processed_tweet = re.sub(r'of|to|https|keep|128', '', processed_tweet)\n",
    "#     processed_tweet = lemmatizer.lemmatize(processed_tweet)\n",
    "# #     lambda x: lemmatizer.lemmatize(x in processed_tweet)\n",
    "#     # Converting to Lowercase\n",
    "#     processed_tweet = processed_tweet.lower()\n",
    "    \n",
    "#     tweet = processed_tweet\n",
    "    \n",
    "    \n",
    "processed_tweets = []\n",
    "X = all_tweets\n",
    "for tweet in range(0, len(X)):  \n",
    "    \n",
    "\n",
    "    processed_tweet = re.sub(r\"http\\S+\", \"\", str(X[tweet]))\n",
    "\n",
    "    # Remove all the special characters\n",
    "    processed_tweet = re.sub(r'\\W', ' ', processed_tweet)\n",
    "    \n",
    "#     processed_tweet = re.sub(r'http\\S+', '', processed_tweet)\n",
    "    \n",
    "    \n",
    "#     processed_tweet = re.sub(r'co\\S+', '', processed_tweet) \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    " \n",
    "    # Remove single characters from the start\n",
    "    processed_tweet = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_tweet) \n",
    " \n",
    "    # Substituting multiple spaces with single space\n",
    "    processed_tweet= re.sub(r'\\s+', ' ', processed_tweet, flags=re.I)\n",
    " \n",
    "    # Removing prefixed 'b'\n",
    "    processed_tweet = re.sub(r'^b\\s+', '', processed_tweet)\n",
    "    \n",
    "\n",
    "    \n",
    "#     processed_tweet = re.sub(r'of|to|https|keep', '', processed_tweet)\n",
    "    # Converting to Lowercase\n",
    "    processed_tweet = processed_tweet.lower()\n",
    "    \n",
    "    processed_tweet = lemming(processed_tweet)\n",
    "    processed_tweet = TreebankWordDetokenizer().detokenize(processed_tweet)    \n",
    "    \n",
    "    # remove all single characters\n",
    "    processed_tweet = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_tweet)\n",
    "    processed_tweets.append(processed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "tamil-composition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020 is almost ended brand new sealed factory unlocked iphone12 mini to one lucky follower keep following amp retweeting itech911 ibuy isell iswap ifix apple 0262666226\n",
      "if you feel like apple ha no love for you by not delivering the heart month challenge badge just restart your watch and wait for few minute you ll get it apple applewatch challenge\n",
      "new rip is live 2w1b 051 iphone capitalism amazon tax gripe mondaythoughts mondaymorning mondaymotivation hamont btc bitcoin iphone12 apple aapl eeftober spotify ps5 investing itunes sony weed cgc etf amzn spot nlpoli\n"
     ]
    }
   ],
   "source": [
    "print(processed_tweets[0])\n",
    "print(processed_tweets[1])\n",
    "print(processed_tweets[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "simple-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t0.10378008268576916\n",
      "  (0, 34)\t0.18419582908637713\n",
      "  (0, 14)\t0.08445848860963202\n",
      "  (0, 30)\t0.09187542928543256\n",
      "  (0, 26)\t0.22443153601406954\n",
      "  (0, 22)\t0.08463426568468516\n",
      "  (0, 9)\t0.2547065677027308\n",
      "  (0, 3)\t0.17119712179269916\n",
      "  (0, 2)\t0.14122564458503742\n",
      "  (0, 16)\t0.17872517137676927\n",
      "  (0, 20)\t0.19891043303145406\n",
      "  (0, 28)\t0.1684994505688957\n",
      "  (0, 33)\t0.1818628830219665\n",
      "  (0, 15)\t0.27180525325058125\n",
      "  (0, 21)\t0.1933259623804885\n",
      "  (0, 25)\t0.22012294324860932\n",
      "  (0, 4)\t0.36187297077110625\n",
      "  (0, 18)\t0.24219945381211117\n",
      "  (0, 24)\t0.22511263095002002\n",
      "  (0, 13)\t0.25298058821310954\n",
      "  (0, 23)\t0.20366114070115587\n",
      "  (0, 0)\t0.27999544406345467\n",
      "  (0, 29)\t0.18492188932933612\n",
      "  (0, 27)\t0.13048352030712054\n",
      "  (1, 32)\t0.1116202834795065\n",
      "  :\t:\n",
      "  (517130, 11)\t0.135839254980182\n",
      "  (517130, 10)\t0.38618704858746866\n",
      "  (517130, 17)\t0.10080806835159276\n",
      "  (517130, 12)\t0.13581868475479603\n",
      "  (517130, 34)\t0.11486642069017963\n",
      "  (517130, 14)\t0.12568418027292302\n",
      "  (517130, 30)\t0.12023887092682785\n",
      "  (517130, 22)\t0.08936227956399884\n",
      "  (517130, 9)\t0.2689355112341541\n",
      "  (517130, 3)\t0.22404846203576428\n",
      "  (517130, 2)\t0.1491150827731199\n",
      "  (517130, 16)\t0.06582703697709459\n",
      "  (517130, 20)\t0.07326155756028746\n",
      "  (517130, 28)\t0.05007022857997376\n",
      "  (517130, 15)\t0.11928133895757953\n",
      "  (517130, 21)\t0.07822601756030617\n",
      "  (517130, 25)\t0.07889578883057372\n",
      "  (517130, 4)\t0.22566772061655335\n",
      "  (517130, 18)\t0.1291018522099511\n",
      "  (517130, 24)\t0.1330367606275614\n",
      "  (517130, 13)\t0.13974232539401965\n",
      "  (517130, 23)\t0.11614446438065258\n",
      "  (517130, 0)\t0.17721169266128065\n",
      "  (517130, 29)\t0.14236298978533266\n",
      "  (517130, 27)\t0.11468279392565967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "tfidfv = TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')\n",
    "df2 = tfidfv.fit_transform(processed_tweets)\n",
    "print(df2)\n",
    "# df2array = df2.toarray()\n",
    "# print(df2array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "described-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 556)\t0.40357727068148264\n",
      "  (0, 88)\t0.34308710016633726\n",
      "  (0, 97)\t0.417333363196858\n",
      "  (0, 443)\t0.3951839877481259\n",
      "  (0, 674)\t0.3663719517401211\n",
      "  (0, 1770)\t0.3199229308458355\n",
      "  (0, 1783)\t0.31050472625527564\n",
      "  (0, 943)\t0.15704072326243243\n",
      "  (0, 1716)\t0.15129041849415864\n",
      "  (0, 1471)\t0.09239185493528781\n"
     ]
    }
   ],
   "source": [
    "print(type(df2))\n",
    "print(type(df2[-1]))\n",
    "print(type(df2[-1][-1]))\n",
    "print(df2[-1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "structural-utility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6813202\n"
     ]
    }
   ],
   "source": [
    "print(df2.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pacific-maine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 943)\t0.1619164850216469\n",
      "  (0, 1667)\t0.24642359750991072\n",
      "  (0, 412)\t0.07984327940748012\n",
      "  (0, 863)\t0.07926270652557654\n",
      "  (0, 957)\t0.3644418579168204\n",
      "  (0, 1148)\t0.37252279088881424\n",
      "  (0, 24)\t0.44364094790152897\n",
      "  (0, 1243)\t0.19111331976332038\n",
      "  (0, 802)\t0.4405447542842643\n",
      "  (0, 1005)\t0.38259748006594846\n",
      "  (0, 1716)\t0.15598764620493955\n",
      "  (0, 189)\t0.14697293203863546\n",
      "  (0, 1471)\t0.09526041452797107\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(df2[0])\n",
    "\n",
    "print(df2array[0])\n",
    "print(type(df2[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df2array))\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "print(df2array[0])\n",
    "np.set_printoptions(threshold = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "abroad-conditioning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rt nicochan33 apple exec chose to keep hack of 128 million iphones quiet tech feedly apple iphone cyberse', 'rt roselovestyle house of the dragon the prequel to game of throne wa announced by casey bloys president of hbo gamesofthrones', 'rt gtorges ich habe jetzt einiges an material zu fuellmich amp co beim applesupport eingereicht hoffentlich verschwindet der podcast', 'rt itech911 future version of apple carkey could detect when it being used near wireless charger and alter how it work to avoid', 'Áô∫Â£≤Êó•„Å´‰∫àÁ¥Ñ„Åó„Å¶„Çà„ÅÜ„ÇÑ„ÅèÂ±ä„ÅÑ„Åü apple airtag „Ç±„Éº„Çπ„ÅØÁ¥îÊ≠£„Åò„ÇÉ„Å™„ÅÑ Âøò„Çå„ÇìÂùä ÂÆü„ÅØËøΩÂä†„Åß„ÇÇ„ÅÜ‰∏Ä„Å§Ë≤∑„Å£„Åü', 'rt nicochan33 apple exec chose to keep hack of 128 million iphones quiet tech feedly apple iphone cyberse', 'rt foxconcours_ apple macbook tente de gagner un macbookpro13 en participant ce concours pour tenter votre chance rt fo', 'rt taisy0 apple apple tv „Åßsiri„ÇíÂà©Áî®ÂèØËÉΩ„Å™ÂõΩ„ÇíÊã°Â§ß Ê∞ó„Å´„Å™„Çã Ë®ò„Å´„Å™„Çã apple', 'rt neodiycom how to fix apple macbookpro 2011 running windows10 10 64 bit audio sound driver problem macbook', 'rt indievideogames dino maker make dinos and level to share indiegame gamedev indiedev retro indievideogames io indiegames vi']\n"
     ]
    }
   ],
   "source": [
    "print(processed_tweets[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "decimal-infrared",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517131\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(len(processed_tweets))\n",
    "print(type(processed_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "precise-worthy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sacred-postcard",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e96957eda679>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# https://programmerbackpack.com/tf-idf-explained-and-python-implementation/\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdfnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtfidfv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"TF-IDF\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdfnew\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'TF-IDF'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdfnew\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "# https://programmerbackpack.com/tf-idf-explained-and-python-implementation/\n",
    "dfnew = pd.DataFrame(df2[0].T.todense(), index=tfidfv.get_feature_names(), columns=[\"TF-IDF\"])\n",
    "dfnew = dfnew.sort_values('TF-IDF', ascending=False)\n",
    "print (dfnew.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "spanish-merchant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517128</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517129</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517130</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>517131 rows √ó 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9     ...  1990  \\\n",
       "0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "1        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "2        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "3        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "4        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...   ...  ...   ...   \n",
       "517126   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517127   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517128   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517129   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "517130   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  ...   0.0   \n",
       "\n",
       "        1991  1992  1993  1994  1995  1996  1997  1998  1999  \n",
       "0        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4        0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "...      ...   ...   ...   ...   ...   ...   ...   ...   ...  \n",
       "517126   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517127   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517128   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517129   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "517130   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[517131 rows x 2000 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Trying an alternative data implementation, I haven't used this in the later stages yet\n",
    "df3 = pd.DataFrame.sparse.from_spmatrix(df2)\n",
    "print(type(df3))\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "healthy-tractor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "#Selecting the data and splitting into train and test\n",
    "y = merged['Change']\n",
    "print(type(y))\n",
    "# X = df3\n",
    "# X = df2\n",
    "# X = df2array\n",
    "X = processed_tweets\n",
    "from sklearn.model_selection import train_test_split  \n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "# train_data = train_data.sample(frac=x)\n",
    "# train_holdout_data.to_csv('train_holdout_data.csv', index=False)\n",
    "# x_holdout.to_csv('x_holdout.csv', index=False)\n",
    "# y_holdout.to_csv('y_holdout.csv', index=False)\n",
    "# train_data.to_csv('train_data.csv', index=False)\n",
    "# # x_train.to_csv('x_train.csv', index=False)\n",
    "# y_train.to_csv('y_train.csv', index=False)\n",
    "# # x_test.to_csv('x_test.csv', index=False)\n",
    "# y_test.to_csv('y_test.csv', index=False)\n",
    "\n",
    "# # train_holdout_data = pd.read_csv('train_holdout_data.csv')\n",
    "# # x_holdout = pd.read_csv('x_holdout.csv')\n",
    "# # y_holdout = pd.read_csv('y_holdout.csv')\n",
    "# # x_train = pd.read_csv('x_train.csv')\n",
    "# y_train = pd.read_csv('y_train.csv')\n",
    "# # x_test = pd.read_csv('x_test.csv')\n",
    "# y_test = pd.read_csv('y_test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fluid-armor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406545\n"
     ]
    }
   ],
   "source": [
    "# y_train = pd.Series(y_train)\n",
    "# y_test = pd.Series(y_test)\n",
    "print(len(x_train))\n",
    "# print(type(y_train.values))\n",
    "# print(np.isnan(X).sum())\n",
    "# print(y.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "north-secretariat",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "413704\n"
     ]
    }
   ],
   "source": [
    "# x_train = torch.from_numpy(x_train)\n",
    "# x_test = torch.from_numpy(x_test)\n",
    "# y_train = torch.from_numpy(y_train.values)\n",
    "# y_test = torch.from_numpy(y_test.values)\n",
    "print(len(x_train))\n",
    "# print(x_train)\n",
    "# print(x_test)\n",
    "# print(y_train)\n",
    "# print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mathematical-offer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-42a2226c82f1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(x_test))\n",
    "print(type(y_train))\n",
    "print(type(y_test))\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)\n",
    "# print(x_test[100000][0])\n",
    "# if 'X' in x_train:\n",
    "#     print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "compound-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = y_train.astype(np.float32)\n",
    "# y_test = y_test.astype(np.float32)\n",
    "y_train = y_train.values.reshape(-1,1)\n",
    "y_test = y_test.values.reshape(-1,1)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "# y_train = y_train.squeeze()\n",
    "# y_test = y_test.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sensitive-roller",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(y_train.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "junior-outdoors",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'dtypes'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-a8b81fe93964>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# x_train1 = x_train.astype(np.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# y_train1 = y_train.astype(np.float32)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'dtypes'"
     ]
    }
   ],
   "source": [
    "# x_train1 = x_train.astype(np.float32)\n",
    "# y_train1 = y_train.astype(np.float32)\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)\n",
    "print(x_train1.dtype)\n",
    "print(y_train1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "working-tuning",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-7197c060c3b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "# for x in y_train[0]:\n",
    "#     if type(x) != \"class 'numpy.float32'\":\n",
    "#         print(\"double\")\n",
    "print(type(x_train[0]))\n",
    "print(type(y_train))\n",
    "print(type(y_train[0]))\n",
    "print(type(y_train[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "matched-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trainarray = y_train.values.tolist()\n",
    "y_testarray = y_test.values.tolist()\n",
    "# y_trainarray = y_train.to_numpy()\n",
    "# y_testarray = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "parental-series",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "tfidfv = TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')\n",
    "x_train1 = tfidfv.fit_transform(x_train)\n",
    "x_test1 = tfidfv.fit_transform(x_test)\n",
    "x_train2 = x_train1.astype(dtype = np.float32)\n",
    "x_test2 = x_test1.astype(dtype = np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alpha-jacksonville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5205\n",
      "(413704, 1)\n",
      "[[ 1.33]\n",
      " [-2.01]\n",
      " [ 1.14]\n",
      " ...\n",
      " [-3.29]\n",
      " [-2.73]\n",
      " [-1.33]]\n"
     ]
    }
   ],
   "source": [
    "print(x_train2.shape[1])\n",
    "print(y_train.shape)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "amber-telephone",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_train))\n",
    "# tokenise me pls\n",
    "a = [word_tokenize(x) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "unlimited-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373108\n"
     ]
    }
   ],
   "source": [
    "def flatten(t):\n",
    "    return [item for sublist in t for item in sublist]\n",
    "print(len(set(flatten(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "weird-cardiff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss      dur\n",
      "-------  ------------  ------------  -------\n",
      "      1        \u001b[36m4.1718\u001b[0m        \u001b[32m4.0988\u001b[0m  74.6012\n",
      "      2        \u001b[36m4.0228\u001b[0m        \u001b[32m4.0428\u001b[0m  70.6258\n",
      "      3        \u001b[36m3.9905\u001b[0m        \u001b[32m4.0125\u001b[0m  68.7844\n",
      "      4        \u001b[36m3.9709\u001b[0m        \u001b[32m3.9934\u001b[0m  69.1349\n",
      "      5        \u001b[36m3.9554\u001b[0m        \u001b[32m3.9805\u001b[0m  69.8406\n",
      "      6        \u001b[36m3.9436\u001b[0m        3.9816  68.5065\n",
      "      7        \u001b[36m3.9315\u001b[0m        \u001b[32m3.9778\u001b[0m  68.6152\n",
      "      8        \u001b[36m3.9206\u001b[0m        \u001b[32m3.9605\u001b[0m  68.8205\n",
      "      9        \u001b[36m3.9104\u001b[0m        3.9722  69.0376\n",
      "     10        \u001b[36m3.9013\u001b[0m        \u001b[32m3.9433\u001b[0m  68.9242\n",
      "     11        \u001b[36m3.8915\u001b[0m        \u001b[32m3.9386\u001b[0m  68.4290\n",
      "     12        \u001b[36m3.8838\u001b[0m        3.9411  69.4425\n",
      "     13        \u001b[36m3.8770\u001b[0m        3.9544  75.4372\n",
      "     14        \u001b[36m3.8697\u001b[0m        \u001b[32m3.9361\u001b[0m  70.9167\n",
      "     15        \u001b[36m3.8632\u001b[0m        3.9429  67.9107\n",
      "     16        \u001b[36m3.8576\u001b[0m        3.9392  68.3335\n",
      "     17        \u001b[36m3.8512\u001b[0m        3.9534  69.3909\n",
      "     18        \u001b[36m3.8461\u001b[0m        3.9376  71.2812\n",
      "Stopping since valid_loss has not improved in the last 5 epochs.\n"
     ]
    }
   ],
   "source": [
    "# import torch.nn.functional as F\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "in_dimension = 413704\n",
    "hid_dimension = 3\n",
    "out_dimension = 1\n",
    "\n",
    "\n",
    "class PoleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_dimension,hid_dimension)\n",
    "        self.fc2 = nn.Linear(hid_dimension,out_dimension)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        hidden = self.fc1(X)\n",
    "        hidden = self.sigmoid(hidden)\n",
    "        output = self.fc2(hidden)\n",
    "        return output\n",
    "\n",
    "from skorch import NeuralNetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "x_trainshape = 5842\n",
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "pole_model = RegressorModule()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(pole_model.parameters(), lr = 0.1)\n",
    "\n",
    "net = NeuralNetRegressor(module=pole_model, max_epochs=30, lr=0.1, callbacks =[('earlystopping',EarlyStopping())])\n",
    "def typechange(x):\n",
    "    return x.astype(dtype = np.float32)\n",
    "typetransform = FunctionTransformer(typechange)\n",
    "def inputneuron(x):\n",
    "    x_trainshape = x.shape[1]\n",
    "#     return x_trainshape\n",
    "inputneuronnumber = FunctionTransformer(inputneuron)\n",
    "# pipe = Pipeline([('transform', DataFrameTransformer()),('net', net)])\n",
    "# pipe = Pipeline([('net', net)])\n",
    "pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"typetransform\", typetransform), (\"net\", net)])\n",
    "\n",
    "\n",
    "# net = skorch.NeuralNetClassifier(module=PoleNN, max_epochs=20, lr=0.1, criterion=torch.nn.NLLLoss)\n",
    "pipe.fit(X=x_train, y=y_train)\n",
    "y_pred = pipe.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grave-vegetarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9780481"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared = False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "arctic-phenomenon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71518.68]\n",
      "[35764.336]\n"
     ]
    }
   ],
   "source": [
    "profit = 0\n",
    "for i, (real, pred) in enumerate(zip(y_test, y_pred)):\n",
    "    if i == 0:\n",
    "        pass\n",
    "    else:\n",
    "        realchange = y_test[i] - y_test[i-1]\n",
    "        invest  = (y_pred[i] - y_pred[i-1])\n",
    "        change = (invest * realchange)\n",
    "        profit += change\n",
    "print(profit)\n",
    "profit = 0           \n",
    "for i, (real, pred) in enumerate(zip(y_test, y_pred)):\n",
    "    realmean = y_test.mean()\n",
    "    predmean = y_pred.mean()\n",
    "    realchange = real - realmean\n",
    "    predchange = pred - predmean\n",
    "    change = (predchange * realchange)\n",
    "    profit += change\n",
    "print(profit)\n",
    "            \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "numerical-priority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<scipy.stats._distn_infrastructure.rv_frozen object at 0x00000246D1FE4D90>\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import loguniform, uniform\n",
    "print(uniform(20, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expensive-testimony",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5018], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5018], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 4994], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 4971], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5041], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5018], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5018], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 4994], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 4971], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5041], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\pipeline.py\", line 346, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\regressor.py\", line 91, in fit\n",
      "    return super(NeuralNetRegressor, self).fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 903, in fit\n",
      "    self.partial_fit(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 862, in partial_fit\n",
      "    self.fit_loop(X, y, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 775, in fit_loop\n",
      "    self.run_single_epoch(dataset_train, training=True, prefix=\"train\",\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 812, in run_single_epoch\n",
      "    step = step_fn(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 709, in train_step\n",
      "    self.optimizer_.step(step_fn)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\optim\\sgd.py\", line 80, in step\n",
      "    loss = closure()\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 705, in step_fn\n",
      "    step = self.train_step_single(Xi, yi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 645, in train_step_single\n",
      "    y_pred = self.infer(Xi, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\skorch\\net.py\", line 1048, in infer\n",
      "    return self.module_(x, **fit_params)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<ipython-input-7-6c6d51de6d20>\", line 43, in forward\n",
      "    X = self.nonlin(self.dense0(X))\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "  File \"D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n",
      "    ret = torch.addmm(bias, input, weight.t())\n",
      "RuntimeError: size mismatch, m1: [128 x 5018], m2: [5204 x 10] at C:\\Users\\builder\\AppData\\Local\\Temp\\pip-req-build-e5c8dddg\\aten\\src\\TH/generic/THTensorMath.cpp:136\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_validate\n",
    "from scipy.stats import loguniform, uniform\n",
    "params = {\n",
    "    'net__lr': [0.001,0.003,0.01,0.03,0.1,0.3,0.6,0.9,1],\n",
    "    'net__max_epochs': [20,40,50],\n",
    "    'net__optimizer__momentum': [0.95,0.96,0.97,0.98,0.99]\n",
    "}\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'net__lr': loguniform(0.001,1),\n",
    "              'net__max_epochs': [20,30,40,50],\n",
    "              'net__optimizer__momentum': loguniform(0.9, 0.99)}\n",
    "\n",
    "pipe = Pipeline([(\"typetransform\", typetransform), (\"net\", net)])\n",
    "tfidf = TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')\n",
    "x_train2 = tfidf.fit_transform(x_train)\n",
    "x_trainshape = x_train2.shape[1]\n",
    "class RegressorModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_units=10,\n",
    "            nonlin=F.relu,\n",
    "    ):\n",
    "        super(RegressorModule, self).__init__()\n",
    "        self.num_units = num_units\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "        self.dense0 = nn.Linear(x_trainshape, num_units)\n",
    "        self.nonlin = nonlin\n",
    "        self.dense1 = nn.Linear(num_units, 10)\n",
    "        self.output = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = self.nonlin(self.dense0(X))\n",
    "        X = F.relu(self.dense1(X))\n",
    "        X = self.output(X)\n",
    "        return X\n",
    "\n",
    "pole_model = RegressorModule()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(pole_model.parameters(), lr = 0.1)\n",
    "\n",
    "net = NeuralNetRegressor(module=pole_model, max_epochs=30, lr=0.1, callbacks =[('earlystopping',EarlyStopping())])\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time.time()\n",
    "random_search.fit(x_train2, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)\n",
    "\n",
    "\n",
    "# gs = GridSearchCV(pipe, params, n_jobs=-1)\n",
    "# gs.fit(X=x_train, y=y_train);\n",
    "# print(gs.best_score_, gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = pd.DataFrame(y_test)\n",
    "dftest['index'] = list(range(103427))\n",
    "dfpred = pd.DataFrame(y_pred)\n",
    "dfpred['index'] = list(range(103427))\n",
    "fix, ax = plt.subplots()\n",
    "sns.kdeplot(x=dftest['index'],y=dftest[0], cumulative=True, color='orange', label='real')\n",
    "sns.kdeplot(x=dfpred['index'],y=dfpred[0], cumulative=True, color='b', label='pred')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "amazing-mason",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 20), (1000, 1), -6.4901485, 6.154505)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_regr, y_regr = make_regression(1000, 20, n_informative=10, random_state=0)\n",
    "X_regr = X_regr.astype(np.float32)\n",
    "y_regr = y_regr.astype(np.float32) / 100\n",
    "y_regr = y_regr.reshape(-1, 1)\n",
    "\n",
    "X_regr.shape, y_regr.shape, y_regr.min(), y_regr.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "diverse-budget",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.float32'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y_regr))\n",
    "print(type(y_regr[0]))\n",
    "print(type(y_regr[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "signal-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing optimizer.\n",
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m4.6416\u001b[0m        \u001b[32m3.8738\u001b[0m  1.3260\n",
      "      2        \u001b[36m4.3003\u001b[0m        \u001b[32m2.7721\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.7503\u001b[0m        \u001b[32m0.4882\u001b[0m  0.0240\n",
      "      4        \u001b[36m0.4048\u001b[0m        \u001b[32m0.1588\u001b[0m  0.0240\n",
      "      5        \u001b[36m0.1568\u001b[0m        0.2447  0.0240\n",
      "      6        0.2712        \u001b[32m0.1374\u001b[0m  0.0260\n",
      "      7        0.1773        0.4521  0.0220\n",
      "      8        0.2750        \u001b[32m0.0977\u001b[0m  0.0250\n",
      "      9        \u001b[36m0.0932\u001b[0m        0.1167  0.0230\n",
      "     10        \u001b[36m0.0787\u001b[0m        \u001b[32m0.0926\u001b[0m  0.0280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=RegressorModule(\n",
       "    (dense0): Linear(in_features=20, out_features=10, bias=True)\n",
       "    (dense1): Linear(in_features=10, out_features=10, bias=True)\n",
       "    (output): Linear(in_features=10, out_features=1, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fit(X=X_regr, y=y_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-evanescence",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "text_classifier = RandomForestRegressor(n_estimators=100, random_state=0)  \n",
    "text_classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "predictions = text_classifier.predict(x_test)\n",
    " \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    " \n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  \n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "meaning-graduate",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'get_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-b9d3a85ec1ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'get_params'"
     ]
    }
   ],
   "source": [
    "cross_val_score.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "lesser-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = merged['Change']\n",
    "\n",
    "X = processed_tweets\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['text'] = processed_tweets\n",
    "data['y'] = y.tolist()\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split  \n",
    "data1 = data.sample(frac=0.2)\n",
    "X1 = data1['text'].tolist()\n",
    "y2 = np.array(data1['y'])\n",
    "x_trainsvm, x_testsvm, y_trainsvm, y_testsvm = train_test_split(X1, y2, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "lovely-casino",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time:  1705.32363986969 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVR, SVR\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "# y_trainsvm = np.array(y_train).ravel()\n",
    "# y_testsvm = np.array(y_test).ravel()\n",
    "\n",
    "#Create the SVM model\n",
    "\n",
    "start = time.time()\n",
    "# regressor = LinearSVR(random_state = 0)\n",
    "regressor = SVR()\n",
    "#Fit the model for the data\n",
    "pipe = Pipeline([(\"tfidf_vector_com\", TfidfVectorizer(tokenizer=identity_tokenizer, input=\"array\", lowercase=False, norm=\"l2\", max_features=None, sublinear_tf=True, stop_words=\"english\",token_pattern=r'[^\\s]+')), (\"regressor\", regressor)])\n",
    "\n",
    "pipe.fit(x_trainsvm, y_trainsvm)\n",
    "\n",
    "#Make the prediction\n",
    "y_predsvm = pipe.predict(x_testsvm)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed Time: \", (end-start),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "going-voltage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.977553460848178\n",
      "R-Squared: 0.0333595799537606\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_testsvm, y_predsvm, squared = False)\n",
    "print(\"RMSE:\",rmse)\n",
    "r2 = r2_score(y_testsvm,y_predsvm)\n",
    "print(\"R-Squared:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-retrieval",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "first-interim",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\anaconda3\\envs\\DataSciEnv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "accuracies = cross_val_score(estimator = regressor, X = x_trainsvm, y = y_trainsvm, cv = 10)\n",
    "end = time.time()\n",
    "# print(\"Fraction: \", x)\n",
    "# print(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\n",
    "# print(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n",
    "mse = mean_squared_error(y_test,y_predsvm)\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "r2 = r2_score(y_test,y_predsvm)\n",
    "print(\"R-squared: {}\".format(r2))\n",
    "print(\"\")\n",
    "print(\"Elapsed Time: \", (end-start),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "australian-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.00098072231255\n",
      "Mean Squared Error: 4.00098072231255\n",
      "R-squared: 0.025518011281076225\n"
     ]
    }
   ],
   "source": [
    "mse = mean_squared_error(y_test,y_predsvm)\n",
    "print(\"Mean Squared Error: {}\".format(mse))\n",
    "r2 = r2_score(y_test,y_predsvm)\n",
    "print(\"R-squared: {}\".format(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-gnome",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:DataSciEnv]",
   "language": "python",
   "name": "conda-env-DataSciEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
